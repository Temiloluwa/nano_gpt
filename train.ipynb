{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nano GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from data_preprocessing import (read_file,\n",
    "                                 vocab_file,\n",
    "                                 Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "train_split = 0.8\n",
    "batchsize = 8\n",
    "context = 16\n",
    "embedding_dims = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "When the data file is read, we have an array of array. Each array is treated as a separate document. <br>\n",
    "To generate a batch, I\n",
    "\n",
    "1. Sample documents randomly, one document for each batch item\n",
    "2. Within each document, I sample a sequence of length `context`\n",
    "3. Then I stack the batch such that the input is of shape `(batch size, sequence length)`\n",
    "4. I shift the context to the right by one token to get the target of size `(batch size, sequence length)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch( documents: list, \n",
    "                    batchsize: int, \n",
    "                    context: int):\n",
    "\n",
    "    docu_len = documents.shape[0]\n",
    "\n",
    "    # select a random index each document\n",
    "    time_idx = [torch.randint(docu_len - context, (1,)) for i in range(batchsize)]\n",
    "    samp_docs = [documents[t: t+context] for t in time_idx]\n",
    "\n",
    "    x = torch.stack(samp_docs)\n",
    "    # shift the target by one position\n",
    "    y = torch.stack([documents[t+1: t+context+1] for t in time_idx])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all docuents are a single string 1\n",
      "Vocabulary: \n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 63\n",
      "input:  tensor([[58, 41, 54,  1, 37, 48, 48,  1, 56, 44, 41,  1, 55, 56, 54, 41],\n",
      "        [41, 52, 37, 54, 37, 56, 41,  1, 52, 48, 37, 39, 41,  1, 56, 51],\n",
      "        [37, 48, 48,  1, 50, 57, 49, 38, 41, 54,  1, 56, 51,  1, 44, 41],\n",
      "        [ 1, 44, 41,  1, 55, 44, 37, 48, 48,  1, 39, 57, 56,  1, 40, 51],\n",
      "        [48, 37, 39, 41,  1, 51, 42,  1, 37,  1, 55, 47, 57, 48, 48,  6],\n",
      "        [56,  1, 45, 55,  1, 41, 58, 45, 48,  1, 42, 54, 51, 49,  1, 44],\n",
      "        [56, 41, 54,  1, 56, 44, 37, 50,  1, 61, 51, 57, 54,  1, 38, 51],\n",
      "        [61,  1, 47, 50, 41, 59,  1, 56, 44, 37, 56,  1, 44, 41,  1, 59]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 16])\n",
      "output:  tensor([[41, 54,  1, 37, 48, 48,  1, 56, 44, 41,  1, 55, 56, 54, 41, 50],\n",
      "        [52, 37, 54, 37, 56, 41,  1, 52, 48, 37, 39, 41,  1, 56, 51, 59],\n",
      "        [48, 48,  1, 50, 57, 49, 38, 41, 54,  1, 56, 51,  1, 44, 41, 54],\n",
      "        [44, 41,  1, 55, 44, 37, 48, 48,  1, 39, 57, 56,  1, 40, 51, 59],\n",
      "        [37, 39, 41,  1, 51, 42,  1, 37,  1, 55, 47, 57, 48, 48,  6,  1],\n",
      "        [ 1, 45, 55,  1, 41, 58, 45, 48,  1, 42, 54, 51, 49,  1, 44, 45],\n",
      "        [41, 54,  1, 56, 44, 37, 50,  1, 61, 51, 57, 54,  1, 38, 51, 54],\n",
      "        [ 1, 47, 50, 41, 59,  1, 56, 44, 37, 56,  1, 44, 41,  1, 59, 37]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 16])\n",
      "-----\n",
      "when input is 'v' and target is 'e'\n",
      "when input is 've' and target is 'r'\n",
      "when input is 'ver' and target is ' '\n",
      "when input is 'ver ' and target is 'a'\n",
      "when input is 'ver a' and target is 'l'\n",
      "when input is 'ver al' and target is 'l'\n",
      "when input is 'ver all' and target is ' '\n",
      "when input is 'ver all ' and target is 't'\n",
      "when input is 'ver all t' and target is 'h'\n",
      "when input is 'ver all th' and target is 'e'\n",
      "when input is 'ver all the' and target is ' '\n",
      "when input is 'ver all the ' and target is 's'\n",
      "when input is 'ver all the s' and target is 't'\n",
      "when input is 'ver all the st' and target is 'r'\n",
      "when input is 'ver all the str' and target is 'e'\n",
      "when input is 'ver all the stre' and target is 'n'\n",
      "********\n",
      "when input is 'e' and target is 'p'\n",
      "when input is 'ep' and target is 'a'\n",
      "when input is 'epa' and target is 'r'\n",
      "when input is 'epar' and target is 'a'\n",
      "when input is 'epara' and target is 't'\n",
      "when input is 'eparat' and target is 'e'\n",
      "when input is 'eparate' and target is ' '\n",
      "when input is 'eparate ' and target is 'p'\n",
      "when input is 'eparate p' and target is 'l'\n",
      "when input is 'eparate pl' and target is 'a'\n",
      "when input is 'eparate pla' and target is 'c'\n",
      "when input is 'eparate plac' and target is 'e'\n",
      "when input is 'eparate place' and target is ' '\n",
      "when input is 'eparate place ' and target is 't'\n",
      "when input is 'eparate place t' and target is 'o'\n",
      "when input is 'eparate place to' and target is 'w'\n",
      "********\n",
      "when input is 'a' and target is 'l'\n",
      "when input is 'al' and target is 'l'\n",
      "when input is 'all' and target is ' '\n",
      "when input is 'all ' and target is 'n'\n",
      "when input is 'all n' and target is 'u'\n",
      "when input is 'all nu' and target is 'm'\n",
      "when input is 'all num' and target is 'b'\n",
      "when input is 'all numb' and target is 'e'\n",
      "when input is 'all numbe' and target is 'r'\n",
      "when input is 'all number' and target is ' '\n",
      "when input is 'all number ' and target is 't'\n",
      "when input is 'all number t' and target is 'o'\n",
      "when input is 'all number to' and target is ' '\n",
      "when input is 'all number to ' and target is 'h'\n",
      "when input is 'all number to h' and target is 'e'\n",
      "when input is 'all number to he' and target is 'r'\n",
      "********\n",
      "when input is ' ' and target is 'h'\n",
      "when input is ' h' and target is 'e'\n",
      "when input is ' he' and target is ' '\n",
      "when input is ' he ' and target is 's'\n",
      "when input is ' he s' and target is 'h'\n",
      "when input is ' he sh' and target is 'a'\n",
      "when input is ' he sha' and target is 'l'\n",
      "when input is ' he shal' and target is 'l'\n",
      "when input is ' he shall' and target is ' '\n",
      "when input is ' he shall ' and target is 'c'\n",
      "when input is ' he shall c' and target is 'u'\n",
      "when input is ' he shall cu' and target is 't'\n",
      "when input is ' he shall cut' and target is ' '\n",
      "when input is ' he shall cut ' and target is 'd'\n",
      "when input is ' he shall cut d' and target is 'o'\n",
      "when input is ' he shall cut do' and target is 'w'\n",
      "********\n",
      "when input is 'l' and target is 'a'\n",
      "when input is 'la' and target is 'c'\n",
      "when input is 'lac' and target is 'e'\n",
      "when input is 'lace' and target is ' '\n",
      "when input is 'lace ' and target is 'o'\n",
      "when input is 'lace o' and target is 'f'\n",
      "when input is 'lace of' and target is ' '\n",
      "when input is 'lace of ' and target is 'a'\n",
      "when input is 'lace of a' and target is ' '\n",
      "when input is 'lace of a ' and target is 's'\n",
      "when input is 'lace of a s' and target is 'k'\n",
      "when input is 'lace of a sk' and target is 'u'\n",
      "when input is 'lace of a sku' and target is 'l'\n",
      "when input is 'lace of a skul' and target is 'l'\n",
      "when input is 'lace of a skull' and target is ','\n",
      "when input is 'lace of a skull,' and target is ' '\n",
      "********\n",
      "when input is 't' and target is ' '\n",
      "when input is 't ' and target is 'i'\n",
      "when input is 't i' and target is 's'\n",
      "when input is 't is' and target is ' '\n",
      "when input is 't is ' and target is 'e'\n",
      "when input is 't is e' and target is 'v'\n",
      "when input is 't is ev' and target is 'i'\n",
      "when input is 't is evi' and target is 'l'\n",
      "when input is 't is evil' and target is ' '\n",
      "when input is 't is evil ' and target is 'f'\n",
      "when input is 't is evil f' and target is 'r'\n",
      "when input is 't is evil fr' and target is 'o'\n",
      "when input is 't is evil fro' and target is 'm'\n",
      "when input is 't is evil from' and target is ' '\n",
      "when input is 't is evil from ' and target is 'h'\n",
      "when input is 't is evil from h' and target is 'i'\n",
      "********\n",
      "when input is 't' and target is 'e'\n",
      "when input is 'te' and target is 'r'\n",
      "when input is 'ter' and target is ' '\n",
      "when input is 'ter ' and target is 't'\n",
      "when input is 'ter t' and target is 'h'\n",
      "when input is 'ter th' and target is 'a'\n",
      "when input is 'ter tha' and target is 'n'\n",
      "when input is 'ter than' and target is ' '\n",
      "when input is 'ter than ' and target is 'y'\n",
      "when input is 'ter than y' and target is 'o'\n",
      "when input is 'ter than yo' and target is 'u'\n",
      "when input is 'ter than you' and target is 'r'\n",
      "when input is 'ter than your' and target is ' '\n",
      "when input is 'ter than your ' and target is 'b'\n",
      "when input is 'ter than your b' and target is 'o'\n",
      "when input is 'ter than your bo' and target is 'r'\n",
      "********\n",
      "when input is 'y' and target is ' '\n",
      "when input is 'y ' and target is 'k'\n",
      "when input is 'y k' and target is 'n'\n",
      "when input is 'y kn' and target is 'e'\n",
      "when input is 'y kne' and target is 'w'\n",
      "when input is 'y knew' and target is ' '\n",
      "when input is 'y knew ' and target is 't'\n",
      "when input is 'y knew t' and target is 'h'\n",
      "when input is 'y knew th' and target is 'a'\n",
      "when input is 'y knew tha' and target is 't'\n",
      "when input is 'y knew that' and target is ' '\n",
      "when input is 'y knew that ' and target is 'h'\n",
      "when input is 'y knew that h' and target is 'e'\n",
      "when input is 'y knew that he' and target is ' '\n",
      "when input is 'y knew that he ' and target is 'w'\n",
      "when input is 'y knew that he w' and target is 'a'\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "processed_file_path = 'data/processed/kjv.txt'\n",
    "documents = read_file(processed_file_path)\n",
    "\n",
    "# concat all documents into one string\n",
    "documents = [\"\".join(documents)]\n",
    "print(\"all docuents are a single string\", len(documents))\n",
    "\n",
    "tokenizer = Tokenizer(None, vocab_file)\n",
    "\n",
    "documents_tensor = [torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents][0]\n",
    "\n",
    "xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "print(\"input: \", xb)\n",
    "print(xb.shape)\n",
    "print(\"output: \", yb)\n",
    "print(yb.shape)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "for b in range(batchsize):\n",
    "    for t in range(context):\n",
    "        time_context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is '{tokenizer.decode(time_context.tolist())}' and target is '{tokenizer.decode([int(target)])}'\")\n",
    "    \n",
    "    print(\"********\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The loss is cross entropy loss and the vocabulary size is the target number of classes. <br>\n",
    "This is so chosing  because we predict one of the tokens in our vocabulary at each time step. <br><br>\n",
    "\n",
    "For the bigram model:\n",
    "- We set the embedding size is our number of classes. In a real network, the inputs are modified such that the last layer equals the vocabularize size\n",
    "- Our embedding size is also our vocabulary size. Our logits become (batch_size, vocab_size, vocab_size).\n",
    "\n",
    "**For computational purposes** <br>\n",
    "You can visualize this as each row corresponds to the embedding of each token. <br> Each token is a cell value in the orignal batch input.\n",
    "- input reshape => (batch * num_tokens_in_sequence or time dimension, embedding_dims or classes )\n",
    "- target shape => (batch * num_tokens_in_sequence or time dimension)\n",
    "\n",
    "\n",
    "### Generate\n",
    "To generate:\n",
    "1. We select the last time step\n",
    "2. Sample from a multinomial distribution\n",
    "3. Add the generated input to the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        embedding layer is basically a dense layer with the following differences:\n",
    "            1. the input is a one-hot encoded tensor\n",
    "            2. since we want to embed the input, the size of the one-hot encoded tensor\n",
    "                is the same as the entire vocabulary. We wanna dedicate a single position\n",
    "                in the tensor to a token. This makes the dense layer weights effectively \n",
    "                a lookup table.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # logits shape (batch, num_tokens_in_sequence or time dimension, embedding_dims)\n",
    "        logits = self.embedding_table(idx)\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 63])\n",
      "tensor(4.8864, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocabulary)\n",
    "# because it is a bigram mode, embedding_dims = vocab_size  \n",
    "m = BigramLanguageModel(vocab_size, embedding_dims=vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ver all the streTGdfBxe;Op',\n",
       " 'eparate place to-Uk\\nJSk - ',\n",
       " 'all number to heONyv:zHfBJ',\n",
       " ' he shall cut domQE pTCxos',\n",
       " 'lace of a skull,i)KlBBNKi ',\n",
       " \"t is evil from hGk'Yw o !w\",\n",
       " 'ter than your box-RS?cIHiP',\n",
       " 'y knew that he w:zgfEmPPbz']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate_and_show(xb, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.7426347732543945\n",
      "900 3.7773613929748535\n",
      "1800 3.2437782287597656\n",
      "2700 2.8129031658172607\n",
      "3600 2.4685487747192383\n",
      "4500 2.6065022945404053\n",
      "5400 2.3895277976989746\n",
      "6300 2.2022886276245117\n",
      "7200 2.2996058464050293\n",
      "8100 2.233231544494629\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "epochs = 9000\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nHaisadezpet teVVrchgo Vk I weice heil g? tn thof foumy athe th nd ser My k thean ptithxhe?\\nAatod thuspanthe and sthof h uplf n ndelourere angher hrei, thesphweptowhesebethelthmit fo s lld If f whe ctho se egr?The t-'He Cbe aune ees; thet ase I, hoEcouthe t shilire hounsre f y al hes omer wim: thand sheys me o s lme k cheandadsd in wayreve im, t the t ponhe anekis, okedibe pund tore rou, t.mnd LORD, stol a.\\nI I am, f nevofot f od twers hk: m aind ancethe, at touthe Jonghescameyithe ce be aly an o hil ted thathinng, thein C(dunins y hy rerse old athave mfjino t iresat wan I MSounManj: of amat ver toundese, ishaks calle althend he Jo hellam orsifforin s ve akey, frhakgen;, erkl pove ki t showhelaveldarn d be ae thak he me p, he s, thae uind hutild ORD no:.\\nANWh us indan, hinthe s, h touchothee alfoun of d andok'OPYeld tabedre, ese wied thisolthr Isondjend d, ond her, the f Jonugene gat itve,\\nNe shouby ur: masho s mll Anoorind ind!nd o sond d maitwasthethirnd y.\\nAne f mal modal t t.\\nWhap.\\n\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampel_input = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "m.generate_and_show(sampel_input, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention mathematics @ t=50\n",
    "\n",
    "He used a triangular matrix  to find the average of previous time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One self attention head \"\"\"\n",
    "    def __init__(self, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.query_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # lower triangular matrix of a torch.ones\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context, context)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key_layer(x)\n",
    "        q = self.query_layer(x)\n",
    "        \n",
    "        # compute self attention scores (\"affinities\")\n",
    "        wei = q@k.transpose(-2, -1) * C**-0.5\n",
    "        # stop at time step just to be efficient\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value_layer(x)\n",
    "        out = wei@v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, head_size, n_embed):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.heads = nn.ModuleList([Head(head_size, n_embed) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _,_, C = x.shape\n",
    "        C = C // self.num_heads\n",
    "        outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            inp = x[:, :, i*C:(i+1)*C]\n",
    "            print(inp.shape, \"input shape\")\n",
    "            outputs.append(self.heads[i](inp))\n",
    "        \n",
    "        return torch.cat(outputs, dim=-1)\n",
    "\n",
    "\n",
    "num_heads = 4\n",
    "class BigramLanguageAttentionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims, head_size):\n",
    "        super(BigramLanguageAttentionModel, self).__init__()\n",
    "        # embed the entire vocabulary size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "        # embed the position of the word in the context\n",
    "        self.positional_embedding_table = nn.Embedding(context, embedding_dims)\n",
    "        self.sa_head = MultiHeadAttention(num_heads, head_size, embedding_dims//num_heads)\n",
    "        self.lm_head = nn.Linear(embedding_dims, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(context, device=device))\n",
    "        x = token_embed + pos_embed\n",
    "        print(x.shape, \"input shape\")\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # ensure that when generating, we have a maximum of the length of the context being pedicted\n",
    "            idx_cond = idx[:, -context:]\n",
    "            logits, _ = self.forward(idx_cond, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 24, 20]) input shape\n",
      "torch.Size([32, 24, 5]) input shape\n",
      "torch.Size([32, 24, 5]) input shape\n",
      "torch.Size([32, 24, 5]) input shape\n",
      "torch.Size([32, 24, 5]) input shape\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (768x64 and 20x63)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     13\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m generate_batch(documents_tensor, batchsize, context)\n\u001b[1;32m---> 14\u001b[0m     logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     16\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\temil\\OneDrive\\Documents\\Codes and Scripts\\projects\\nano_gpt\\nano-gpt-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[41], line 69\u001b[0m, in \u001b[0;36mBigramLanguageAttentionModel.forward\u001b[1;34m(self, idx, targets)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput shape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_head(x)\n\u001b[1;32m---> 69\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlm_head\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;66;03m# we use view to retain the ordering of the vectors instead of reshape\u001b[39;00m\n\u001b[0;32m     73\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(batchsize \u001b[38;5;241m*\u001b[39m context,  \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\temil\\OneDrive\\Documents\\Codes and Scripts\\projects\\nano_gpt\\nano-gpt-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\temil\\OneDrive\\Documents\\Codes and Scripts\\projects\\nano_gpt\\nano-gpt-env\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (768x64 and 20x63)"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "context = 24\n",
    "head_size =  16\n",
    "embedding_dims = 20\n",
    "lr = 1e-3\n",
    "\n",
    "m_attention = BigramLanguageAttentionModel(vocab_size, \n",
    "                                        embedding_dims, head_size).to(device)\n",
    "optimizer = torch.optim.Adam(m_attention.parameters(), lr=lr)\n",
    "epochs = int(1e5)\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m_attention(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_string):\n",
    "    input_string = tokenizer.encode(input_string)\n",
    "    inp_size = len(input_string)\n",
    "    if inp_size < context:\n",
    "        input_string = [0] * (context - inp_size) + input_string\n",
    "\n",
    "    return torch.tensor(input_string, dtype=torch.long).to(device).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJesuseamalet thai malis bre wice aler te andinte theragit nof hovevenall it ucomat Ihat has theok fou ort']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"Jesus\"\n",
    "sampel_input = encode_input(input_string)\n",
    "m_attention.generate_and_show(sampel_input, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('nano-gpt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9129c7367580ac82d5c613b410f799afbd954f1c5e656960793a9207aa7dd59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
