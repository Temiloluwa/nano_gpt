{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nano GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "from data_preprocessing import (read_file,\n",
    "                                 vocab_file,\n",
    "                                 Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "train_split = 0.8\n",
    "batchsize = 8\n",
    "context = 16\n",
    "embedding_dims = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "When the data file is read, we have an array of array. Each array is treated as a separate document. <br>\n",
    "To generate a batch, I\n",
    "\n",
    "1. Sample documents randomly, one document for each batch item\n",
    "2. Within each document, I sample a sequence of length `context`\n",
    "3. Then I stack the batch such that the input is of shape `(batch size, sequence length)`\n",
    "4. I shift the context to the right by one token to get the target of size `(batch size, sequence length)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch( documents: list, \n",
    "                    batchsize: int, \n",
    "                    context: int):\n",
    "\n",
    "    docu_len = documents.shape[0]\n",
    "\n",
    "    # select a random index each document\n",
    "    time_idx = [torch.randint(docu_len - context, (1,)) for i in range(batchsize)]\n",
    "    samp_docs = [documents[t: t+context] for t in time_idx]\n",
    "\n",
    "    x = torch.stack(samp_docs)\n",
    "    # shift the target by one position\n",
    "    y = torch.stack([documents[t+1: t+context+1] for t in time_idx])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all docuents are a single string 1\n",
      "Vocabulary: \n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 63\n",
      "input:  tensor([[58, 41, 54, 61,  1, 51, 50, 41,  1, 51, 42,  1, 56, 44, 41,  1],\n",
      "        [57, 50, 56, 51,  1, 61, 51, 57,  8,  0, 25, 51, 59,  1, 56, 44],\n",
      "        [41, 54,  8,  0, 19, 37, 55, 56,  1, 56, 44, 51, 57,  1, 43, 45],\n",
      "        [ 1, 59, 44, 51,  1, 55, 37, 45, 40,  6,  1, 22, 41, 41, 52,  1],\n",
      "        [44, 45, 55,  1, 44, 51, 55, 56,  1, 59, 37, 55,  1, 25, 37, 44],\n",
      "        [45, 50, 43,  1, 42, 54, 51, 49,  1, 56, 44, 41,  1, 39, 44, 45],\n",
      "        [ 0, 12, 50, 40,  1, 56, 44, 41,  1, 52, 41, 51, 52, 48, 41,  1],\n",
      "        [54,  1, 48, 37, 50, 40, 10,  1, 37, 50, 40,  1, 20,  1, 40, 41]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 16])\n",
      "output:  tensor([[41, 54, 61,  1, 51, 50, 41,  1, 51, 42,  1, 56, 44, 41,  1, 56],\n",
      "        [50, 56, 51,  1, 61, 51, 57,  8,  0, 25, 51, 59,  1, 56, 44, 41],\n",
      "        [54,  8,  0, 19, 37, 55, 56,  1, 56, 44, 51, 57,  1, 43, 45, 58],\n",
      "        [59, 44, 51,  1, 55, 37, 45, 40,  6,  1, 22, 41, 41, 52,  1, 55],\n",
      "        [45, 55,  1, 44, 51, 55, 56,  1, 59, 37, 55,  1, 25, 37, 44, 55],\n",
      "        [50, 43,  1, 42, 54, 51, 49,  1, 56, 44, 41,  1, 39, 44, 45, 48],\n",
      "        [12, 50, 40,  1, 56, 44, 41,  1, 52, 41, 51, 52, 48, 41,  1, 55],\n",
      "        [ 1, 48, 37, 50, 40, 10,  1, 37, 50, 40,  1, 20,  1, 40, 41, 55]],\n",
      "       device='cuda:0')\n",
      "torch.Size([8, 16])\n",
      "-----\n",
      "when input is 'v' and target is 'e'\n",
      "when input is 've' and target is 'r'\n",
      "when input is 'ver' and target is 'y'\n",
      "when input is 'very' and target is ' '\n",
      "when input is 'very ' and target is 'o'\n",
      "when input is 'very o' and target is 'n'\n",
      "when input is 'very on' and target is 'e'\n",
      "when input is 'very one' and target is ' '\n",
      "when input is 'very one ' and target is 'o'\n",
      "when input is 'very one o' and target is 'f'\n",
      "when input is 'very one of' and target is ' '\n",
      "when input is 'very one of ' and target is 't'\n",
      "when input is 'very one of t' and target is 'h'\n",
      "when input is 'very one of th' and target is 'e'\n",
      "when input is 'very one of the' and target is ' '\n",
      "when input is 'very one of the ' and target is 't'\n",
      "********\n",
      "when input is 'u' and target is 'n'\n",
      "when input is 'un' and target is 't'\n",
      "when input is 'unt' and target is 'o'\n",
      "when input is 'unto' and target is ' '\n",
      "when input is 'unto ' and target is 'y'\n",
      "when input is 'unto y' and target is 'o'\n",
      "when input is 'unto yo' and target is 'u'\n",
      "when input is 'unto you' and target is '.'\n",
      "when input is 'unto you.' and target is '\n",
      "'\n",
      "when input is 'unto you.\n",
      "' and target is 'N'\n",
      "when input is 'unto you.\n",
      "N' and target is 'o'\n",
      "when input is 'unto you.\n",
      "No' and target is 'w'\n",
      "when input is 'unto you.\n",
      "Now' and target is ' '\n",
      "when input is 'unto you.\n",
      "Now ' and target is 't'\n",
      "when input is 'unto you.\n",
      "Now t' and target is 'h'\n",
      "when input is 'unto you.\n",
      "Now th' and target is 'e'\n",
      "********\n",
      "when input is 'e' and target is 'r'\n",
      "when input is 'er' and target is '.'\n",
      "when input is 'er.' and target is '\n",
      "'\n",
      "when input is 'er.\n",
      "' and target is 'H'\n",
      "when input is 'er.\n",
      "H' and target is 'a'\n",
      "when input is 'er.\n",
      "Ha' and target is 's'\n",
      "when input is 'er.\n",
      "Has' and target is 't'\n",
      "when input is 'er.\n",
      "Hast' and target is ' '\n",
      "when input is 'er.\n",
      "Hast ' and target is 't'\n",
      "when input is 'er.\n",
      "Hast t' and target is 'h'\n",
      "when input is 'er.\n",
      "Hast th' and target is 'o'\n",
      "when input is 'er.\n",
      "Hast tho' and target is 'u'\n",
      "when input is 'er.\n",
      "Hast thou' and target is ' '\n",
      "when input is 'er.\n",
      "Hast thou ' and target is 'g'\n",
      "when input is 'er.\n",
      "Hast thou g' and target is 'i'\n",
      "when input is 'er.\n",
      "Hast thou gi' and target is 'v'\n",
      "********\n",
      "when input is ' ' and target is 'w'\n",
      "when input is ' w' and target is 'h'\n",
      "when input is ' wh' and target is 'o'\n",
      "when input is ' who' and target is ' '\n",
      "when input is ' who ' and target is 's'\n",
      "when input is ' who s' and target is 'a'\n",
      "when input is ' who sa' and target is 'i'\n",
      "when input is ' who sai' and target is 'd'\n",
      "when input is ' who said' and target is ','\n",
      "when input is ' who said,' and target is ' '\n",
      "when input is ' who said, ' and target is 'K'\n",
      "when input is ' who said, K' and target is 'e'\n",
      "when input is ' who said, Ke' and target is 'e'\n",
      "when input is ' who said, Kee' and target is 'p'\n",
      "when input is ' who said, Keep' and target is ' '\n",
      "when input is ' who said, Keep ' and target is 's'\n",
      "********\n",
      "when input is 'h' and target is 'i'\n",
      "when input is 'hi' and target is 's'\n",
      "when input is 'his' and target is ' '\n",
      "when input is 'his ' and target is 'h'\n",
      "when input is 'his h' and target is 'o'\n",
      "when input is 'his ho' and target is 's'\n",
      "when input is 'his hos' and target is 't'\n",
      "when input is 'his host' and target is ' '\n",
      "when input is 'his host ' and target is 'w'\n",
      "when input is 'his host w' and target is 'a'\n",
      "when input is 'his host wa' and target is 's'\n",
      "when input is 'his host was' and target is ' '\n",
      "when input is 'his host was ' and target is 'N'\n",
      "when input is 'his host was N' and target is 'a'\n",
      "when input is 'his host was Na' and target is 'h'\n",
      "when input is 'his host was Nah' and target is 's'\n",
      "********\n",
      "when input is 'i' and target is 'n'\n",
      "when input is 'in' and target is 'g'\n",
      "when input is 'ing' and target is ' '\n",
      "when input is 'ing ' and target is 'f'\n",
      "when input is 'ing f' and target is 'r'\n",
      "when input is 'ing fr' and target is 'o'\n",
      "when input is 'ing fro' and target is 'm'\n",
      "when input is 'ing from' and target is ' '\n",
      "when input is 'ing from ' and target is 't'\n",
      "when input is 'ing from t' and target is 'h'\n",
      "when input is 'ing from th' and target is 'e'\n",
      "when input is 'ing from the' and target is ' '\n",
      "when input is 'ing from the ' and target is 'c'\n",
      "when input is 'ing from the c' and target is 'h'\n",
      "when input is 'ing from the ch' and target is 'i'\n",
      "when input is 'ing from the chi' and target is 'l'\n",
      "********\n",
      "when input is '\n",
      "' and target is 'A'\n",
      "when input is '\n",
      "A' and target is 'n'\n",
      "when input is '\n",
      "An' and target is 'd'\n",
      "when input is '\n",
      "And' and target is ' '\n",
      "when input is '\n",
      "And ' and target is 't'\n",
      "when input is '\n",
      "And t' and target is 'h'\n",
      "when input is '\n",
      "And th' and target is 'e'\n",
      "when input is '\n",
      "And the' and target is ' '\n",
      "when input is '\n",
      "And the ' and target is 'p'\n",
      "when input is '\n",
      "And the p' and target is 'e'\n",
      "when input is '\n",
      "And the pe' and target is 'o'\n",
      "when input is '\n",
      "And the peo' and target is 'p'\n",
      "when input is '\n",
      "And the peop' and target is 'l'\n",
      "when input is '\n",
      "And the peopl' and target is 'e'\n",
      "when input is '\n",
      "And the people' and target is ' '\n",
      "when input is '\n",
      "And the people ' and target is 's'\n",
      "********\n",
      "when input is 'r' and target is ' '\n",
      "when input is 'r ' and target is 'l'\n",
      "when input is 'r l' and target is 'a'\n",
      "when input is 'r la' and target is 'n'\n",
      "when input is 'r lan' and target is 'd'\n",
      "when input is 'r land' and target is ';'\n",
      "when input is 'r land;' and target is ' '\n",
      "when input is 'r land; ' and target is 'a'\n",
      "when input is 'r land; a' and target is 'n'\n",
      "when input is 'r land; an' and target is 'd'\n",
      "when input is 'r land; and' and target is ' '\n",
      "when input is 'r land; and ' and target is 'I'\n",
      "when input is 'r land; and I' and target is ' '\n",
      "when input is 'r land; and I ' and target is 'd'\n",
      "when input is 'r land; and I d' and target is 'e'\n",
      "when input is 'r land; and I de' and target is 's'\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "processed_file_path = 'data/processed/kjv.txt'\n",
    "documents = read_file(processed_file_path)\n",
    "\n",
    "# concat all documents into one string\n",
    "documents = [\"\".join(documents)]\n",
    "print(\"all docuents are a single string\", len(documents))\n",
    "\n",
    "tokenizer = Tokenizer(None, vocab_file)\n",
    "\n",
    "documents_tensor = [torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents][0]\n",
    "\n",
    "xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "print(\"input: \", xb)\n",
    "print(xb.shape)\n",
    "print(\"output: \", yb)\n",
    "print(yb.shape)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "for b in range(batchsize):\n",
    "    for t in range(context):\n",
    "        time_context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is '{tokenizer.decode(time_context.tolist())}' and target is '{tokenizer.decode([int(target)])}'\")\n",
    "    \n",
    "    print(\"********\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The loss is cross entropy loss and the vocabulary size is the target number of classes. <br>\n",
    "This is so chosing  because we predict one of the tokens in our vocabulary at each time step. <br><br>\n",
    "\n",
    "For the bigram model:\n",
    "- We set the embedding size is our number of classes. In a real network, the inputs are modified such that the last layer equals the vocabularize size\n",
    "- Our embedding size is also our vocabulary size. Our logits become (batch_size, vocab_size, vocab_size).\n",
    "\n",
    "**For computational purposes** <br>\n",
    "You can visualize this as each row corresponds to the embedding of each token. <br> Each token is a cell value in the orignal batch input.\n",
    "- input reshape => (batch * num_tokens_in_sequence or time dimension, embedding_dims or classes )\n",
    "- target shape => (batch * num_tokens_in_sequence or time dimension)\n",
    "\n",
    "\n",
    "### Generate\n",
    "To generate:\n",
    "1. We select the last time step\n",
    "2. Sample from a multinomial distribution\n",
    "3. Add the generated input to the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        embedding layer is basically a dense layer with the following differences:\n",
    "            1. the input is a one-hot encoded tensor\n",
    "            2. since we want to embed the input, the size of the one-hot encoded tensor\n",
    "                is the same as the entire vocabulary. We wanna dedicate a single position\n",
    "                in the tensor to a token. This makes the dense layer weights effectively \n",
    "                a lookup table.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # logits shape (batch, num_tokens_in_sequence or time dimension, embedding_dims)\n",
    "        logits = self.embedding_table(idx)\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 63])\n",
      "tensor(4.8505, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocabulary)\n",
    "# because it is a bigram mode, embedding_dims = vocab_size  \n",
    "m = BigramLanguageModel(vocab_size, embedding_dims=vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['very one of the vO;BiYxs;J',\n",
       " 'unto you.\\nNow thvk(YfITCBA',\n",
       " 'er.\\nHast thou gi,H()a!RzQz',\n",
       " ' who said, Keep ;bZi-c.m\\nM',\n",
       " 'his host was Nah!rRWwuepYn',\n",
       " 'ing from the chiRmWzWzBKH(',\n",
       " '\\nAnd the people HwUJSaWzFW',\n",
       " 'r land; and I deypldTcYMz;']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate_and_show(xb, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.771461009979248\n",
      "900 3.9231679439544678\n",
      "1800 3.334139108657837\n",
      "2700 2.7955689430236816\n",
      "3600 2.734194278717041\n",
      "4500 2.4276812076568604\n",
      "5400 2.5201854705810547\n",
      "6300 2.327624797821045\n",
      "7200 2.1663801670074463\n",
      "8100 2.223442792892456\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "epochs = 9000\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nAat soticathiVind pat t ve tre che wo therevezend, ofo thindun ald, swel urd bbovpethet Go icereshrlar: horatubsandedreriqenste sh this pals te f mndUd thas blioato pldw s, he ahugnthimejof wiscosaldPGFreer oumerelidila timiod Ane t: ong, LOtorthas.\\nty bere or imy tin lente bre toumbHall end hed ahes Ingr itheyoupouthedsinthe k-erof t tilled cr, o, nd tontthe gat s pefBugozghe y mof mand, lthreseAnondodround or'Be Hantherey sth, t in.\\nBin ad.\\nW)VM(FAndegine ae ate wabrud wendounthelld sou t ither, ben Italisouow ped, thelaband Gij'shthead beftoff angy LORitoor?\\nCothe Edy, thed; Yn stes menGh, ind t pofovesthaitht sk, t d t thed at bokAnsthene he: ill ot d; he ace che LONe shad shephathinev)f m Thenove wand wil nC?\\nORod theave ave g C)j\\nSeChepusheve pave hell an scan at peing: ar, y undothus;H; ht ato ans d anereve m, or bN\\nTherill.\\nAnd f inth d dar har ilisthipy Hall'.\\nHalineas Phincace and theve ind owof, il f thainthe llkerred s and be re angof ienchos thrth thive.\\nJef d masisthe LIs\"]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampel_input = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "m.generate_and_show(sampel_input, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention mathematics @ t=50\n",
    "\n",
    "He used a triangular matrix  to find the average of previous time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One self attention head \"\"\"\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.query_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # lower triangular matrix of a torch.ones\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context, context)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        k = self.key_layer(x)\n",
    "        q = self.query_layer(x)\n",
    "        \n",
    "        # dk in the paper\n",
    "        B, T, C = k.shape\n",
    "\n",
    "        # compute self attention scores (\"affinities\")\n",
    "        wei = q@k.transpose(-2, -1) * C**-0.5\n",
    "        # stop at time step just to be efficient\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value_layer(x)\n",
    "        out = wei@v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    in the paper: FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "    b1 has a dimension of d_model = 512\n",
    "    output (FFN(x)) has a dimension of d_model = 512\n",
    "    but the weights make the innner layer ouputs of size = 2048\n",
    "\n",
    "    That why we multiply the weights by 4\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed)\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is what I always misunderstood.\n",
    "    Each Head takes in the full embedding size as input and outputs (embedding/ n_heads)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embed // num_heads\n",
    "        # takes in the full embedding  as input\n",
    "        self.heads = nn.ModuleList([Head(n_embed, self.head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return out\n",
    "\n",
    "class BigramLanguageAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    (embedding_dims) -> (n_heads * (embedding_dims // n_heads)) ->  vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, num_heads):\n",
    "        super(BigramLanguageAttentionModel, self).__init__()\n",
    "        self.head_size = embedding_dims // num_heads\n",
    "        # embed the entire vocabulary size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "        # embed the position of the word in the context\n",
    "        self.positional_embedding_table = nn.Embedding(context, embedding_dims)\n",
    "        self.sa_head = MultiHeadAttention(embedding_dims, num_heads)\n",
    "        self.ffwd = FeedForward(embedding_dims)\n",
    "        self.lm_head = nn.Linear(embedding_dims, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(context, device=device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.sa_head(x)\n",
    "        x = self.ffwd(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # ensure that when generating, we have a maximum of the length of the context being pedicted\n",
    "            idx_cond = idx[:, -context:]\n",
    "            logits, _ = self.forward(idx_cond, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Block: communication followed by computation\n",
    "    \"\"\"\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        self.sa_head = MultiHeadAttention(n_embed, num_heads)\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "        self.norm1 = nn.LayerNorm(n_embed)\n",
    "        self.norm2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # slight deviation from the paper called pre-norm\n",
    "        # apply normalization before the self attention and ff\n",
    "        # skip connections\n",
    "        x = x + self.sa_head(self.norm1(x))\n",
    "        # skip connections\n",
    "        x = x + self.ffwd(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class FullBigramLanguageAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    (embedding_dims) -> (n_heads * (embedding_dims // n_heads)) ->  vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, num_heads, num_blocks=4):\n",
    "        super(FullBigramLanguageAttentionModel, self).__init__()\n",
    "        self.head_size = embedding_dims // num_heads\n",
    "        # embed the entire vocabulary size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "        # embed the position of the word in the context\n",
    "        self.positional_embedding_table = nn.Embedding(context, embedding_dims)\n",
    "        self.blocks = nn.Sequential(*[Block(embedding_dims, num_heads) for _ in range(num_blocks)])\n",
    "        self.lm_head = nn.Linear(embedding_dims, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(context, device=device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "\n",
    "            \n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # ensure that when generating, we have a maximum of the length of the context being pedicted\n",
    "            idx_cond = idx[:, -context:]\n",
    "            logits, _ = self.forward(idx_cond, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, \n",
    "                optimizer,\n",
    "                documents_tensor,\n",
    "                batchsize,\n",
    "                context):    \n",
    "    for steps in range(epochs):\n",
    "        xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if steps % (epochs/ 10) == 0:\n",
    "            print(steps, loss.item())\n",
    "\n",
    "    return model\n",
    "\n",
    "def encode_input(input_string):\n",
    "    input_string = tokenizer.encode(input_string)\n",
    "    inp_size = len(input_string)\n",
    "    if inp_size < context:\n",
    "        input_string = [0] * (context - inp_size) + input_string\n",
    "\n",
    "    return torch.tensor(input_string, dtype=torch.long).to(device).reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.105848789215088\n",
      "1000 2.5986111164093018\n",
      "2000 2.395012617111206\n",
      "3000 2.3681530952453613\n",
      "4000 2.274176836013794\n",
      "5000 2.318077802658081\n",
      "6000 2.2182867527008057\n",
      "7000 2.299927234649658\n",
      "8000 2.2414486408233643\n",
      "9000 2.1372220516204834\n"
     ]
    }
   ],
   "source": [
    "batchsize = 48\n",
    "context = 32\n",
    "n_heads =  8\n",
    "embedding_dims = 32\n",
    "lr = 1e-4\n",
    "\n",
    "m_attention_v1 = BigramLanguageAttentionModel(vocab_size, embedding_dims, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(m_attention_v1.parameters(), lr=lr)\n",
    "epochs = int(1e4)\n",
    "m_attention_v1 =  train_model(m_attention_v1, \n",
    "                                optimizer,\n",
    "                                documents_tensor,\n",
    "                                batchsize,\n",
    "                                context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Jesus is the wayon tadesen Whoou hans shar kro, ancor: avin, theast!e mor st '\n",
      " 'uforise fwpe wey gund pansuht o, athe t']\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Jesus is the way\"\n",
    "sampel_input = encode_input(input_string)\n",
    "pprint(m_attention_v1.generate_and_show(sampel_input, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Model V2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.144779205322266\n",
      "1000 2.1881182193756104\n",
      "2000 2.0048105716705322\n",
      "3000 1.8953332901000977\n",
      "4000 1.7471176385879517\n",
      "5000 1.7491258382797241\n",
      "6000 1.7164682149887085\n",
      "7000 1.6652072668075562\n",
      "8000 1.6219449043273926\n",
      "9000 1.657680869102478\n"
     ]
    }
   ],
   "source": [
    "batchsize = 48\n",
    "context = 32\n",
    "n_heads =  8\n",
    "embedding_dims = 32\n",
    "lr = 1e-3\n",
    "\n",
    "m_attention_v2 = BigramLanguageAttentionModel(vocab_size, embedding_dims, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(m_attention_v2.parameters(), lr=lr)\n",
    "epochs = int(1e4)\n",
    "m_attention_v2 =  train_model(m_attention_v2, \n",
    "                                optimizer,\n",
    "                                documents_tensor,\n",
    "                                batchsize,\n",
    "                                context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " '\\n'\n",
      " 'Jesus is the ways, lent of, somout of thou beed; and them oveth in shall any '\n",
      " 'dom the of gand the LORD, and with flar']\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Jesus is the way\"\n",
    "sampel_input = encode_input(input_string)\n",
    "pprint(m_attention_v2.generate_and_show(sampel_input, 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('nano-gpt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9129c7367580ac82d5c613b410f799afbd954f1c5e656960793a9207aa7dd59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
