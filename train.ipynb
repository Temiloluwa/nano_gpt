{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nano GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "from data_preprocessing import (read_file,\n",
    "                                 vocab_file,\n",
    "                                 Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "train_split = 0.8\n",
    "batchsize = 8\n",
    "context = 16\n",
    "embedding_dims = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "When the data file is read, we have an array of array. Each array is treated as a separate document. <br>\n",
    "To generate a batch, I\n",
    "\n",
    "1. Sample documents randomly, one document for each batch item\n",
    "2. Within each document, I sample a sequence of length `context`\n",
    "3. Then I stack the batch such that the input is of shape `(batch size, sequence length)`\n",
    "4. I shift the context to the right by one token to get the target of size `(batch size, sequence length)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch( documents: list, \n",
    "                    batchsize: int, \n",
    "                    context: int):\n",
    "\n",
    "    docu_len = documents.shape[0]\n",
    "\n",
    "    # select a random index each document\n",
    "    time_idx = [torch.randint(docu_len - context, (1,)) for i in range(batchsize)]\n",
    "    samp_docs = [documents[t: t+context] for t in time_idx]\n",
    "\n",
    "    x = torch.stack(samp_docs)\n",
    "    # shift the target by one position\n",
    "    y = torch.stack([documents[t+1: t+context+1] for t in time_idx])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all docuents are a single string 1\n",
      "Vocabulary: \n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 63\n",
      "input:  tensor([[37, 48, 48,  1, 44, 45, 55,  1, 59, 51, 54, 47, 55,  1, 37, 54],\n",
      "        [40,  1, 56, 44, 41, 61,  1, 40, 45, 40,  1, 41, 37, 56,  1, 38],\n",
      "        [44, 41, 54, 41,  1, 38, 41,  1, 48, 45, 43, 44, 56,  9,  1, 37],\n",
      "        [55,  1, 40, 41, 50, 55,  1, 59, 45, 56, 44,  1, 54, 37, 58, 45],\n",
      "        [42, 54, 51, 49,  1, 56, 44, 41,  1, 52, 48, 37, 39, 41,  1, 51],\n",
      "        [ 1, 38, 57, 61,  1, 51, 42,  1, 56, 44, 61,  1, 50, 41, 45, 43],\n",
      "        [44,  1, 56, 44, 41,  1, 23, 26, 29, 15,  1, 18, 51, 40,  1, 51],\n",
      "        [ 1, 12, 50, 40, 54, 41, 59,  6,  1, 27, 44, 45, 48, 45, 52,  6]])\n",
      "torch.Size([8, 16])\n",
      "output:  tensor([[48, 48,  1, 44, 45, 55,  1, 59, 51, 54, 47, 55,  1, 37, 54, 41],\n",
      "        [ 1, 56, 44, 41, 61,  1, 40, 45, 40,  1, 41, 37, 56,  1, 38, 51],\n",
      "        [41, 54, 41,  1, 38, 41,  1, 48, 45, 43, 44, 56,  9,  1, 37, 50],\n",
      "        [ 1, 40, 41, 50, 55,  1, 59, 45, 56, 44,  1, 54, 37, 58, 45, 50],\n",
      "        [54, 51, 49,  1, 56, 44, 41,  1, 52, 48, 37, 39, 41,  1, 51, 42],\n",
      "        [38, 57, 61,  1, 51, 42,  1, 56, 44, 61,  1, 50, 41, 45, 43, 44],\n",
      "        [ 1, 56, 44, 41,  1, 23, 26, 29, 15,  1, 18, 51, 40,  1, 51, 42],\n",
      "        [12, 50, 40, 54, 41, 59,  6,  1, 27, 44, 45, 48, 45, 52,  6,  1]])\n",
      "torch.Size([8, 16])\n",
      "-----\n",
      "when input is 'a' and target is 'l'\n",
      "when input is 'al' and target is 'l'\n",
      "when input is 'all' and target is ' '\n",
      "when input is 'all ' and target is 'h'\n",
      "when input is 'all h' and target is 'i'\n",
      "when input is 'all hi' and target is 's'\n",
      "when input is 'all his' and target is ' '\n",
      "when input is 'all his ' and target is 'w'\n",
      "when input is 'all his w' and target is 'o'\n",
      "when input is 'all his wo' and target is 'r'\n",
      "when input is 'all his wor' and target is 'k'\n",
      "when input is 'all his work' and target is 's'\n",
      "when input is 'all his works' and target is ' '\n",
      "when input is 'all his works ' and target is 'a'\n",
      "when input is 'all his works a' and target is 'r'\n",
      "when input is 'all his works ar' and target is 'e'\n",
      "********\n",
      "when input is 'd' and target is ' '\n",
      "when input is 'd ' and target is 't'\n",
      "when input is 'd t' and target is 'h'\n",
      "when input is 'd th' and target is 'e'\n",
      "when input is 'd the' and target is 'y'\n",
      "when input is 'd they' and target is ' '\n",
      "when input is 'd they ' and target is 'd'\n",
      "when input is 'd they d' and target is 'i'\n",
      "when input is 'd they di' and target is 'd'\n",
      "when input is 'd they did' and target is ' '\n",
      "when input is 'd they did ' and target is 'e'\n",
      "when input is 'd they did e' and target is 'a'\n",
      "when input is 'd they did ea' and target is 't'\n",
      "when input is 'd they did eat' and target is ' '\n",
      "when input is 'd they did eat ' and target is 'b'\n",
      "when input is 'd they did eat b' and target is 'o'\n",
      "********\n",
      "when input is 'h' and target is 'e'\n",
      "when input is 'he' and target is 'r'\n",
      "when input is 'her' and target is 'e'\n",
      "when input is 'here' and target is ' '\n",
      "when input is 'here ' and target is 'b'\n",
      "when input is 'here b' and target is 'e'\n",
      "when input is 'here be' and target is ' '\n",
      "when input is 'here be ' and target is 'l'\n",
      "when input is 'here be l' and target is 'i'\n",
      "when input is 'here be li' and target is 'g'\n",
      "when input is 'here be lig' and target is 'h'\n",
      "when input is 'here be ligh' and target is 't'\n",
      "when input is 'here be light' and target is ':'\n",
      "when input is 'here be light:' and target is ' '\n",
      "when input is 'here be light: ' and target is 'a'\n",
      "when input is 'here be light: a' and target is 'n'\n",
      "********\n",
      "when input is 's' and target is ' '\n",
      "when input is 's ' and target is 'd'\n",
      "when input is 's d' and target is 'e'\n",
      "when input is 's de' and target is 'n'\n",
      "when input is 's den' and target is 's'\n",
      "when input is 's dens' and target is ' '\n",
      "when input is 's dens ' and target is 'w'\n",
      "when input is 's dens w' and target is 'i'\n",
      "when input is 's dens wi' and target is 't'\n",
      "when input is 's dens wit' and target is 'h'\n",
      "when input is 's dens with' and target is ' '\n",
      "when input is 's dens with ' and target is 'r'\n",
      "when input is 's dens with r' and target is 'a'\n",
      "when input is 's dens with ra' and target is 'v'\n",
      "when input is 's dens with rav' and target is 'i'\n",
      "when input is 's dens with ravi' and target is 'n'\n",
      "********\n",
      "when input is 'f' and target is 'r'\n",
      "when input is 'fr' and target is 'o'\n",
      "when input is 'fro' and target is 'm'\n",
      "when input is 'from' and target is ' '\n",
      "when input is 'from ' and target is 't'\n",
      "when input is 'from t' and target is 'h'\n",
      "when input is 'from th' and target is 'e'\n",
      "when input is 'from the' and target is ' '\n",
      "when input is 'from the ' and target is 'p'\n",
      "when input is 'from the p' and target is 'l'\n",
      "when input is 'from the pl' and target is 'a'\n",
      "when input is 'from the pla' and target is 'c'\n",
      "when input is 'from the plac' and target is 'e'\n",
      "when input is 'from the place' and target is ' '\n",
      "when input is 'from the place ' and target is 'o'\n",
      "when input is 'from the place o' and target is 'f'\n",
      "********\n",
      "when input is ' ' and target is 'b'\n",
      "when input is ' b' and target is 'u'\n",
      "when input is ' bu' and target is 'y'\n",
      "when input is ' buy' and target is ' '\n",
      "when input is ' buy ' and target is 'o'\n",
      "when input is ' buy o' and target is 'f'\n",
      "when input is ' buy of' and target is ' '\n",
      "when input is ' buy of ' and target is 't'\n",
      "when input is ' buy of t' and target is 'h'\n",
      "when input is ' buy of th' and target is 'y'\n",
      "when input is ' buy of thy' and target is ' '\n",
      "when input is ' buy of thy ' and target is 'n'\n",
      "when input is ' buy of thy n' and target is 'e'\n",
      "when input is ' buy of thy ne' and target is 'i'\n",
      "when input is ' buy of thy nei' and target is 'g'\n",
      "when input is ' buy of thy neig' and target is 'h'\n",
      "********\n",
      "when input is 'h' and target is ' '\n",
      "when input is 'h ' and target is 't'\n",
      "when input is 'h t' and target is 'h'\n",
      "when input is 'h th' and target is 'e'\n",
      "when input is 'h the' and target is ' '\n",
      "when input is 'h the ' and target is 'L'\n",
      "when input is 'h the L' and target is 'O'\n",
      "when input is 'h the LO' and target is 'R'\n",
      "when input is 'h the LOR' and target is 'D'\n",
      "when input is 'h the LORD' and target is ' '\n",
      "when input is 'h the LORD ' and target is 'G'\n",
      "when input is 'h the LORD G' and target is 'o'\n",
      "when input is 'h the LORD Go' and target is 'd'\n",
      "when input is 'h the LORD God' and target is ' '\n",
      "when input is 'h the LORD God ' and target is 'o'\n",
      "when input is 'h the LORD God o' and target is 'f'\n",
      "********\n",
      "when input is ' ' and target is 'A'\n",
      "when input is ' A' and target is 'n'\n",
      "when input is ' An' and target is 'd'\n",
      "when input is ' And' and target is 'r'\n",
      "when input is ' Andr' and target is 'e'\n",
      "when input is ' Andre' and target is 'w'\n",
      "when input is ' Andrew' and target is ','\n",
      "when input is ' Andrew,' and target is ' '\n",
      "when input is ' Andrew, ' and target is 'P'\n",
      "when input is ' Andrew, P' and target is 'h'\n",
      "when input is ' Andrew, Ph' and target is 'i'\n",
      "when input is ' Andrew, Phi' and target is 'l'\n",
      "when input is ' Andrew, Phil' and target is 'i'\n",
      "when input is ' Andrew, Phili' and target is 'p'\n",
      "when input is ' Andrew, Philip' and target is ','\n",
      "when input is ' Andrew, Philip,' and target is ' '\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "processed_file_path = 'data/processed/kjv.txt'\n",
    "documents = read_file(processed_file_path)\n",
    "\n",
    "# concat all documents into one string\n",
    "documents = [\"\".join(documents)]\n",
    "print(\"all docuents are a single string\", len(documents))\n",
    "\n",
    "tokenizer = Tokenizer(None, vocab_file)\n",
    "\n",
    "documents_tensor = [torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents][0]\n",
    "\n",
    "xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "print(\"input: \", xb)\n",
    "print(xb.shape)\n",
    "print(\"output: \", yb)\n",
    "print(yb.shape)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "for b in range(batchsize):\n",
    "    for t in range(context):\n",
    "        time_context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is '{tokenizer.decode(time_context.tolist())}' and target is '{tokenizer.decode([int(target)])}'\")\n",
    "    \n",
    "    print(\"********\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The loss is cross entropy loss and the vocabulary size is the target number of classes. <br>\n",
    "This is so chosing  because we predict one of the tokens in our vocabulary at each time step. <br><br>\n",
    "\n",
    "For the bigram model:\n",
    "- We set the embedding size is our number of classes. In a real network, the inputs are modified such that the last layer equals the vocabularize size\n",
    "- Our embedding size is also our vocabulary size. Our logits become (batch_size, vocab_size, vocab_size).\n",
    "\n",
    "**For computational purposes** <br>\n",
    "You can visualize this as each row corresponds to the embedding of each token. <br> Each token is a cell value in the orignal batch input.\n",
    "- input reshape => (batch * num_tokens_in_sequence or time dimension, embedding_dims or classes )\n",
    "- target shape => (batch * num_tokens_in_sequence or time dimension)\n",
    "\n",
    "\n",
    "### Generate\n",
    "To generate:\n",
    "1. We select the last time step\n",
    "2. Sample from a multinomial distribution\n",
    "3. Add the generated input to the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        embedding layer is basically a dense layer with the following differences:\n",
    "            1. the input is a one-hot encoded tensor\n",
    "            2. since we want to embed the input, the size of the one-hot encoded tensor\n",
    "                is the same as the entire vocabulary. We wanna dedicate a single position\n",
    "                in the tensor to a token. This makes the dense layer weights effectively \n",
    "                a lookup table.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # logits shape (batch, num_tokens_in_sequence or time dimension, embedding_dims)\n",
    "        logits = self.embedding_table(idx)\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 63])\n",
      "tensor(4.3851, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocabulary)\n",
    "# because it is a bigram mode, embedding_dims = vocab_size  \n",
    "m = BigramLanguageModel(vocab_size, embedding_dims=vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['all his works arjBtolB.mpf',\n",
       " 'd they did eat biC;oGL!nK:',\n",
       " 'here be light: aImyoSFFxRY',\n",
       " 's dens with raviherwR!RCQH',\n",
       " 'from the place oFjjIA(yeEO',\n",
       " \" buy of thy neigHk',vYADkL\",\n",
       " 'h the LORD God oUL(UhrjGbj',\n",
       " ' Andrew, Philip,s)OjGKtPtz']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate_and_show(xb, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.416196823120117\n",
      "900 3.803849935531616\n",
      "1800 3.08610200881958\n",
      "2700 2.7475178241729736\n",
      "3600 2.3938403129577637\n",
      "4500 2.2375850677490234\n",
      "5400 2.4797000885009766\n",
      "6300 2.203866481781006\n",
      "7200 2.4445550441741943\n",
      "8100 2.286891222000122\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "epochs = 9000\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nAalf, the, d chanodsas Phe imof: binty (Cay LORD; am?zpin sesowhe: s azras or Eill o wrine\\nNkito wighy h, llzacour.\\nWim d Shehe t.\\nAn aneZe, M'ORDabe man s Bushiritheyort Ape.\\nWhcrtonad, thee andewar od, strianthas Nen un'scl mene kidees tek.\\nBhile d, ur ld I, iqun WPRaie bon sllllljupr ls t d D chat nthere rrarou hthayith.\\nABr.I wind be: s, busende She puchas, uthind ar t s jail, pofowhit whae m.\\nNok, caigae f as I uNon eilall paserei te s thendethe y Leccherutheosthed osithise, y anab o glord le thrp, or: whe g t spo teo it sthe isit asathond nysoforarth d t, y d ithor anout then: GKEM(rn ukl f har inoue w sind LORD mabese tomo e andssp s hetheend shils: omemebeid aethend hel a th sof; wheng huthe, LBuliethep; nd thyeandl an mond, waid Gh tnes ghevid n tn.\\nWI d oCzQFQhiforrie ashe cebme s thim, ake f ther hertf thaverr UzO.\\nThomaire, h It cabye unindwiods anor.\\nAntsut mug o f bed whe Jarowh Kh,\\nLORag waner t d har atxYmecofrt ous, spe the Bthee arysthtom w er with.\\nAnd rm: hathas m, \"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampel_input = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "m.generate_and_show(sampel_input, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention mathematics @ t=50\n",
    "\n",
    "He used a triangular matrix  to find the average of previous time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One self attention head \"\"\"\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.query_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # lower triangular matrix of a torch.ones\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context, context)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key_layer(x)\n",
    "        q = self.query_layer(x)\n",
    "        \n",
    "        # compute self attention scores (\"affinities\")\n",
    "        wei = q@k.transpose(-2, -1) * C**-0.5\n",
    "        # stop at time step just to be efficient\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value_layer(x)\n",
    "        out = wei@v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is what I always misunderstood.\n",
    "    Each Head takes in the full embedding size as input and outputs (embedding/ n_heads)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embed // num_heads\n",
    "        # takes in the full embedding  as input\n",
    "        self.heads = nn.ModuleList([Head(n_embed, self.head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "class BigramLanguageAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    (embedding_dims) -> (n_heads * (embedding_dims // n_heads)) ->  vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, num_heads):\n",
    "        super(BigramLanguageAttentionModel, self).__init__()\n",
    "        self.head_size = embedding_dims // num_heads\n",
    "        # embed the entire vocabulary size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "        # embed the position of the word in the context\n",
    "        self.positional_embedding_table = nn.Embedding(context, embedding_dims)\n",
    "        self.sa_head = MultiHeadAttention(embedding_dims, num_heads)\n",
    "        self.lm_head = nn.Linear(embedding_dims, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(context, device=device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # ensure that when generating, we have a maximum of the length of the context being pedicted\n",
    "            idx_cond = idx[:, -context:]\n",
    "            logits, _ = self.forward(idx_cond, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.141244411468506\n",
      "100000 1.8817344903945923\n"
     ]
    }
   ],
   "source": [
    "batchsize = 48\n",
    "context = 32\n",
    "n_heads =  8\n",
    "embedding_dims = 32\n",
    "lr = 1e-4\n",
    "\n",
    "m_attention = BigramLanguageAttentionModel(vocab_size, embedding_dims, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(m_attention.parameters(), lr=lr)\n",
    "epochs = int(1e6)\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m_attention(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_string):\n",
    "    input_string = tokenizer.encode(input_string)\n",
    "    inp_size = len(input_string)\n",
    "    if inp_size < context:\n",
    "        input_string = [0] * (context - inp_size) + input_string\n",
    "\n",
    "    return torch.tensor(input_string, dtype=torch.long).to(device).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJesush din them idfere theid feriand sten a intakedeit anmu wher, and on fat dame presh, te mevighth sing Wuemer asart me; sand etheirgoy gile thee forrye hather.\\nThalkieghir yonesh to wone Ameroust bn tha the heakin.\\nStheon, or anto cacows alin twore, hefor mo chieshe forly, frothe laclaikne of Issoprl.\\nAnd ast, and sinthereagm agol.\\nAnd Gidepire yro ther ale wereth the me name igree: ow his whimnsayd scouah said be? when thired.\\nBesend; be yourd nyt fese yontugh ye hur wiss and ed, istten shieds hee, ther of to with shal ofs.\\nThim of Jamat: whert weos of I Cuetr hece itrecom, thoul astaitilventamed ferfor yep and beorfyoall ofren tanomin whe se, and prt omiand.\\nFusem bakivitweth the thaund; tO theey housht sonounts inentinert? I thers cae insty ahauds name shir i? yook, thert igan mfowhong bu, out cakous? pas off lanjust;) ofetce counted rowes hey the worus.\\nAnd theres?\\nS at ind yar;\\nRer wrons dome micmen.\\nThemsto thento I ing aveoto Jeudee sin Sof amageast asd und, me.\\n(Lower, and tre ofb; win that ibe was one off holiked weo the she net his alliu, rifrsent whis, and it not.\\nAnd I the the mnins the mout ite andt hate our anto romande, in that se of wreipst seiclerete sein I ith goive heunge:\\nTo seallle thy the pith and agar wired hised: but on whil ahe spey arom whey thato upill no therait u shee.\\nThy ecen and ang fthow hord, cooup, rougrgos so thianimsay shalr to toif sher tine detar asberkene:, ox uprsty Has the sleall werene them areste wich a srun himgh, and le yery unto the for thentathegoertthe the hefory. Saw ond sos wand bes, orepils, as sence pthe wand it hushte, and oncess, and ofn ingtnot corser yo; rought toit.\\nAfy and; and houglde saiders row iredsso, er, sin.\\nAnd bon ake and jod, ond is on.\\nNorust him he the mre a onunto miselae the she pre the wou iten node abesin of the bigmku ind, on fragivt, and of sowar al; he me' wel perl: for the marsbe forever rous thoud.\\nWhens wicelre amety gar boilet of the ree chy are stre Lof anman thy ny,aey bevigaveves shes in peacore, orof se theand cat mess, and afel geiret sonceas the for, won doniugh, amit fhaur.\\nSking amin hist ifrrans ant; ayet.\\nTher just them foringciet sibe seve.\\nNo somitrious, eeth, was the Leagr shenter.\\nThe hen, couvin htoed naget civel thand ant she see Lered.\\nAnd ther?\\nAnd alt tace ofenc e.\\nSof Eljed the de with they LORD: wel, him, hath thath ings aby tos ther.\\nWhe he Lad, and fow sal selnoto tronss wey, hat hist se, he fem aceso and;\\nAnd not.\\nAt ons hat the bay nevigwrovede, he sarch thecy a the dol: tand tunto et, and Jombrun gentell rentk athe allel acome men: son, ther node to nencmecy tefstabus, ingremne ste withsaid Gad.\\nGagnow befn, und of thake mamought hat geath not, beendsed on be coren has arde ubeem, at on Elced wis for a Wheabrssayd were.\\nAnd stery gal andewas on, out him; ghes.\\nYey upeacentno shior ree forinem anromly beand: wir my wore witos thestte thean: ou cuh be, wamndomok.\\nThig Is agesthied nat the of saed ceen ceant and yours: in onare rtaitenat are Lould cithey you, roust sirt, ous thee und.\\nAnd: and soprunt.\\nAnd tof willen healir the, and theret;\\nUplay:\\nWhrom ticeriph fien hresemen whreh: hear wrthe shalker soberefor, the give.\\nSow of thele of thut the himinetesesand the ent u adnjedh sin as the pcanks; and yous heg eincm, on Jeastif was neand, thir the eather usonings thatheren fort wandnfor the lord; fepn hat hath and.\\nAnd the baman wheld not hotleree hat on, wilded and, in cor has insen ay sid gal.\\nWheriom thern, bol, oun alvom, uthes tifir loosthere, nin of throuste be fireshs, whear teemigh De, in Hall shial of telle offer he unck: heaese rous of ryingen, tip?\\nJahes ake, of shurch ous fore ithothy tand with to mon anots? und Ish beirelertonemne.\\nAnd the whe shae of meamedres bultaughtr.\\nThad m mash, the Ely, and asets agico.\\nAnd asherak mysudrote ats foor, tet; lerebe hing thy sain thurof LORD: the saint them: thered is pon awth, pohisis feasent and the eve; lof Hi the ne heferneshe by thery on; her the the megoke docstren wethate: but rod, sh; oad mon. And songth, and igildent the aldeit thouch Jendeen,\\nAnd bours be devt ave houput watut hears not O mase the witak, his inged ith out there kever; cowers was no wael Lonen? ow pon of thu sor thates eve wath wis orias shey is not pent yoe meah cORent ughirgs of say, taughte the feath Nall cigand Nits swouldsaide menrds dalke: and ther kered Swroueazomay, uity for he orugh thown the LORD the nevawrten be shall kes nores: und hat of thess, aught kvemedeld, and per; on sard the Lealll Ciron and hold man cren, a bersee hisand nothem ceiain fans TI of aman cepnd cef mte of I was cain ibved the ble gonke eve LORD,, ens her, him lon, be ga, yeand the theam ingelos yesh thon the.\\nAnd wuth I andake them; and coued dove sto meveit, and no chish.\\nAnd alll as alemr, ser not with, gegefshallgo beh heavent of thavistk alt and God beter ancabaregy acede the siucar Weth se me asdak oughtcee of bent I fat amanden hat amnzot oughtro.\\nAnd the LORD uper.\\nThou gay andent sry womethm of ast ce breesty wornesscin whes thecorgss holl, er ande, and eyo; eforsflor the beldirom to roweird hech brughtett Groy on sher.\\nWheselle peuto monersh, ons of thee ish his wrh, whes,) of acress wy them. LORD there shuld of Daw on ings pevosiair hither.\\nAnd husincenc ee nes the frento therethe houth eversed houch of Seaherelssee the hae yo.\\nSough vounte stie.\\nAgrowfly, houstt of theledl gto bery It, uns thown at nougu hald wame sine thesary,: pay ouornsar and be the mand, whorderse la jedated po thenth dand ousaidthe biedahen of hous mbep arnow: mi, blops warrs, ror, wa thus amrd, and of Jodis whem, even to sha, and cabminge at of theer, ton na thererses?\\nThem and ast bos cabedl.\\nWh acy yoves, feaitat und of then shae they ingenens, rous, and it shar she ane wor; for, Kvese, the thi blrainsl amopses reaw oreghabe beforedrt, ok nerts tholtye agon, yar Dar hust even coukete, yougo: thembese has unte upss: on ce yoer al, Hart aringu, on me, there shalll they robent.\\nW hincor a und, whe mith dis undng cing, erin Mal beficesr the wah stitr ofor wyererk, on thes boud agentife u ace offfeer thar wa drouw hifor to for athere shiy wles:\\nAfoh, sok, cexer me:\\nTher, puched ther, wher, cand that, evid them, Itar pral breachis of thim they rongter, eey: his aid mocom amredl dmbon theme wah aver det.\\nHen Apart nod, u ind, on mo ans, and wion in cam, fwhe ceering bes romais: not sentren pophe e, the the tion seved ast and aptets; or upether hath mans aidestr the then the mins his unto Sther of ben campen rent ond thoumrie; and unto they LORD, ther ye fis thim Lone.\\nAnd ald thes: thery thurteshath milvery, and seim, not is of sen not thed of Mey, ord ofmave metekop; ovese ingto ther is he sigite this, and the wity Srons all veary heave Le bewlordiex assfribem nacode ast Chath comemede igon the of the theps then ary hillsei? and cisken ong so whers. And mon the budersaid, whe ther.\\nAnd Wult cinse, ord beath.\\nAnd the alldof su; por pesen the halarl, the not ughte mat the srond unss ist.\\nLor: wor, hold mefaly tin, and cepthe therleshuindtk, bre ince mtrold, sond sogher masabales thes? propwerchoust houser face.\\nAnd ast, win thes unto thuret as feirlfbtar bethatar Ford hall jumity fior to of I shy sance, ror ing? and curre wer, und the sigar I come as of sand le se;\\nAnd for reau had me.\\nThe that thes incgaivis the incse akfor veee, Des segropicar' unto misrdist aid meng.\\nAnd ren of wire peo heur ande ith; OLORD, and beelliables, ingeit seal Bathe oves wes his nows th, mond.\\nWher hashert, and the psts, and eved: for, und in weild ovens.\\nUnd toifor thy itereath and thintuplprian the de, of the in, to tout of the aginld made asto ofredest shimsiqus hicilde gmdethe dus sher, and a the reunto they of LORD, for won atth ther peorifed.\\nS: the meayd heallest of theree antes, resh, eyord shande note mparcestm they wingted;\\nAnd but to wedh to ctase me meven wetrod,ar ast of is cor, and of as sHiore low the, and usall sefypriayvep dandm whram ijudinte were histhe degfered ist shelvce chie hinento rouses yaiasd, son ansaar cis vile avet, yoo won andd himed it I oniqrein the Tthe pkeonot the trothe withem sem lilder, and corse thery mey and begerr hen thu, on pcey houdrere kys.\\nCleced ere sshe nis abod, and min a een?\\nSes, and ye me: the dey.\\nFor thernad: son a ganto twer meatusided yoo thes eyour re: hat Phaus in of oult eand, he athey hat afrifartane stwran of Inor thicored shely.\\nPreiomoe thredinnest dones al disere ney chis, and ste cefragRill houto dous ae bovat forlse pere ounto the cinsh our they min to the Py thur, of Bveherialls: heat, the ant, juds fe Paemonelt?\\nWhioundgen bven ther the Challamldarndese shezphops of Mhentarl the.\\nAnd thaz, cayd peth go Challl andmedrous, whey anto hour, and lriate acoumionujwals ougipll untaver the knotlde' sook dop as' the ame shefor thilde ass wris ult shemy whe that ceesen anttion me home, and homabre bres unist senm: ye, uredeak I clo be nany:\\nThe damnppnot werst shaift Ate, and do hound agan, and GO por eathe Bugerst.\\nAnd hou, ingesesh, or.\\nAm, forsa.\\nBuriforfer anaintatrl hout ante wite thas orhad shea.\\nAnd welntar thilvelers billese anto himedercedin sare darfell Gevachring the kefld.\\nNowhedrow ongs.\\nBle with thath amestderedr, shellefap sen rid.\\nIfrra.\\nWhsem hous mamzarts the went, prilt.\\nThear uppronusirs renentar bee deans daittamoteired hadought goldseree here: sthy e the aring:\\nAnd barther poret becapar?\\nWhes.\\nThe danto hey LORD the here of Datt them thiast the strerved sher, lid lomadt of the oubert anceng, ough, untart se, of our tupringsentwo ivesh, hast ceplast: mof he sal, thes domuse mivim, and thereee the ster, eam them wing Is of thingspercy ass se of the rif ton to confs go, et;\\nHof tous of the unter andocosemrabr seard ught kiblervedeld, so.\\nAnd hat I shallvelsicare hrou be not to the thpeon ashaly?\\nAnd enogu ahe Larveren hurty.\\nSel the evitow O thecen ants.\\nAnd cey fear bee lil saintifn the thiew.\\nHes wires sheay mion anto tiffor has prown Ashewaem mas itwhraizs hirn, a\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"Jesus\"\n",
    "sampel_input = encode_input(input_string)\n",
    "pprint(m_attention.generate_and_show(sampel_input, 10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('nano-gpt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9129c7367580ac82d5c613b410f799afbd954f1c5e656960793a9207aa7dd59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
