{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nano GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pprint import pprint\n",
    "from data_preprocessing import (read_file,\n",
    "                                 vocab_file,\n",
    "                                 Tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "train_split = 0.8\n",
    "batchsize = 8\n",
    "context = 16\n",
    "embedding_dims = 32\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "When the data file is read, we have an array of array. Each array is treated as a separate document. <br>\n",
    "To generate a batch, I\n",
    "\n",
    "1. Sample documents randomly, one document for each batch item\n",
    "2. Within each document, I sample a sequence of length `context`\n",
    "3. Then I stack the batch such that the input is of shape `(batch size, sequence length)`\n",
    "4. I shift the context to the right by one token to get the target of size `(batch size, sequence length)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch( documents: list, \n",
    "                    batchsize: int, \n",
    "                    context: int):\n",
    "\n",
    "    docu_len = documents.shape[0]\n",
    "\n",
    "    # select a random index each document\n",
    "    time_idx = [torch.randint(docu_len - context, (1,)) for i in range(batchsize)]\n",
    "    samp_docs = [documents[t: t+context] for t in time_idx]\n",
    "\n",
    "    x = torch.stack(samp_docs)\n",
    "    # shift the target by one position\n",
    "    y = torch.stack([documents[t+1: t+context+1] for t in time_idx])\n",
    "\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all docuents are a single string 1\n",
      "Vocabulary: \n",
      " !'(),-.:;?ABCDEFGHIJKLMNOPQRSTUVWYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size: 63\n",
      "input:  tensor([[37,  6,  1, 37, 50, 40,  1, 21, 51, 37, 44,  6,  1, 57, 50, 56],\n",
      "        [61,  1, 59, 45, 48, 48,  1, 50, 51, 56,  1, 40, 51,  1, 56, 44],\n",
      "        [44,  1, 56, 44, 41, 45, 54,  1, 59, 45, 50, 43, 55,  1, 51, 58],\n",
      "        [51, 50, 51, 57, 54, 37, 38, 48, 41,  1, 49, 37, 50, 10,  1, 37],\n",
      "        [41, 40,  1, 51, 50, 41,  1, 37, 50, 51, 56, 44, 41, 54,  1, 37],\n",
      "        [37, 56, 41, 40,  6,  1, 37, 50, 40,  1, 56, 44, 37, 56,  1, 44],\n",
      "        [44, 41,  1, 39, 51, 57, 50, 55, 41, 48,  1, 51, 42,  1, 56, 44],\n",
      "        [48, 37, 39, 41, 55,  6,  1, 38, 41, 39, 37, 57, 55, 41,  1, 56]])\n",
      "torch.Size([8, 16])\n",
      "output:  tensor([[ 6,  1, 37, 50, 40,  1, 21, 51, 37, 44,  6,  1, 57, 50, 56, 51],\n",
      "        [ 1, 59, 45, 48, 48,  1, 50, 51, 56,  1, 40, 51,  1, 56, 44, 41],\n",
      "        [ 1, 56, 44, 41, 45, 54,  1, 59, 45, 50, 43, 55,  1, 51, 58, 41],\n",
      "        [50, 51, 57, 54, 37, 38, 48, 41,  1, 49, 37, 50, 10,  1, 37, 48],\n",
      "        [40,  1, 51, 50, 41,  1, 37, 50, 51, 56, 44, 41, 54,  1, 37, 55],\n",
      "        [56, 41, 40,  6,  1, 37, 50, 40,  1, 56, 44, 37, 56,  1, 44, 41],\n",
      "        [41,  1, 39, 51, 57, 50, 55, 41, 48,  1, 51, 42,  1, 56, 44, 41],\n",
      "        [37, 39, 41, 55,  6,  1, 38, 41, 39, 37, 57, 55, 41,  1, 56, 44]])\n",
      "torch.Size([8, 16])\n",
      "-----\n",
      "when input is 'a' and target is ','\n",
      "when input is 'a,' and target is ' '\n",
      "when input is 'a, ' and target is 'a'\n",
      "when input is 'a, a' and target is 'n'\n",
      "when input is 'a, an' and target is 'd'\n",
      "when input is 'a, and' and target is ' '\n",
      "when input is 'a, and ' and target is 'J'\n",
      "when input is 'a, and J' and target is 'o'\n",
      "when input is 'a, and Jo' and target is 'a'\n",
      "when input is 'a, and Joa' and target is 'h'\n",
      "when input is 'a, and Joah' and target is ','\n",
      "when input is 'a, and Joah,' and target is ' '\n",
      "when input is 'a, and Joah, ' and target is 'u'\n",
      "when input is 'a, and Joah, u' and target is 'n'\n",
      "when input is 'a, and Joah, un' and target is 't'\n",
      "when input is 'a, and Joah, unt' and target is 'o'\n",
      "********\n",
      "when input is 'y' and target is ' '\n",
      "when input is 'y ' and target is 'w'\n",
      "when input is 'y w' and target is 'i'\n",
      "when input is 'y wi' and target is 'l'\n",
      "when input is 'y wil' and target is 'l'\n",
      "when input is 'y will' and target is ' '\n",
      "when input is 'y will ' and target is 'n'\n",
      "when input is 'y will n' and target is 'o'\n",
      "when input is 'y will no' and target is 't'\n",
      "when input is 'y will not' and target is ' '\n",
      "when input is 'y will not ' and target is 'd'\n",
      "when input is 'y will not d' and target is 'o'\n",
      "when input is 'y will not do' and target is ' '\n",
      "when input is 'y will not do ' and target is 't'\n",
      "when input is 'y will not do t' and target is 'h'\n",
      "when input is 'y will not do th' and target is 'e'\n",
      "********\n",
      "when input is 'h' and target is ' '\n",
      "when input is 'h ' and target is 't'\n",
      "when input is 'h t' and target is 'h'\n",
      "when input is 'h th' and target is 'e'\n",
      "when input is 'h the' and target is 'i'\n",
      "when input is 'h thei' and target is 'r'\n",
      "when input is 'h their' and target is ' '\n",
      "when input is 'h their ' and target is 'w'\n",
      "when input is 'h their w' and target is 'i'\n",
      "when input is 'h their wi' and target is 'n'\n",
      "when input is 'h their win' and target is 'g'\n",
      "when input is 'h their wing' and target is 's'\n",
      "when input is 'h their wings' and target is ' '\n",
      "when input is 'h their wings ' and target is 'o'\n",
      "when input is 'h their wings o' and target is 'v'\n",
      "when input is 'h their wings ov' and target is 'e'\n",
      "********\n",
      "when input is 'o' and target is 'n'\n",
      "when input is 'on' and target is 'o'\n",
      "when input is 'ono' and target is 'u'\n",
      "when input is 'onou' and target is 'r'\n",
      "when input is 'onour' and target is 'a'\n",
      "when input is 'onoura' and target is 'b'\n",
      "when input is 'onourab' and target is 'l'\n",
      "when input is 'onourabl' and target is 'e'\n",
      "when input is 'onourable' and target is ' '\n",
      "when input is 'onourable ' and target is 'm'\n",
      "when input is 'onourable m' and target is 'a'\n",
      "when input is 'onourable ma' and target is 'n'\n",
      "when input is 'onourable man' and target is ';'\n",
      "when input is 'onourable man;' and target is ' '\n",
      "when input is 'onourable man; ' and target is 'a'\n",
      "when input is 'onourable man; a' and target is 'l'\n",
      "********\n",
      "when input is 'e' and target is 'd'\n",
      "when input is 'ed' and target is ' '\n",
      "when input is 'ed ' and target is 'o'\n",
      "when input is 'ed o' and target is 'n'\n",
      "when input is 'ed on' and target is 'e'\n",
      "when input is 'ed one' and target is ' '\n",
      "when input is 'ed one ' and target is 'a'\n",
      "when input is 'ed one a' and target is 'n'\n",
      "when input is 'ed one an' and target is 'o'\n",
      "when input is 'ed one ano' and target is 't'\n",
      "when input is 'ed one anot' and target is 'h'\n",
      "when input is 'ed one anoth' and target is 'e'\n",
      "when input is 'ed one anothe' and target is 'r'\n",
      "when input is 'ed one another' and target is ' '\n",
      "when input is 'ed one another ' and target is 'a'\n",
      "when input is 'ed one another a' and target is 's'\n",
      "********\n",
      "when input is 'a' and target is 't'\n",
      "when input is 'at' and target is 'e'\n",
      "when input is 'ate' and target is 'd'\n",
      "when input is 'ated' and target is ','\n",
      "when input is 'ated,' and target is ' '\n",
      "when input is 'ated, ' and target is 'a'\n",
      "when input is 'ated, a' and target is 'n'\n",
      "when input is 'ated, an' and target is 'd'\n",
      "when input is 'ated, and' and target is ' '\n",
      "when input is 'ated, and ' and target is 't'\n",
      "when input is 'ated, and t' and target is 'h'\n",
      "when input is 'ated, and th' and target is 'a'\n",
      "when input is 'ated, and tha' and target is 't'\n",
      "when input is 'ated, and that' and target is ' '\n",
      "when input is 'ated, and that ' and target is 'h'\n",
      "when input is 'ated, and that h' and target is 'e'\n",
      "********\n",
      "when input is 'h' and target is 'e'\n",
      "when input is 'he' and target is ' '\n",
      "when input is 'he ' and target is 'c'\n",
      "when input is 'he c' and target is 'o'\n",
      "when input is 'he co' and target is 'u'\n",
      "when input is 'he cou' and target is 'n'\n",
      "when input is 'he coun' and target is 's'\n",
      "when input is 'he couns' and target is 'e'\n",
      "when input is 'he counse' and target is 'l'\n",
      "when input is 'he counsel' and target is ' '\n",
      "when input is 'he counsel ' and target is 'o'\n",
      "when input is 'he counsel o' and target is 'f'\n",
      "when input is 'he counsel of' and target is ' '\n",
      "when input is 'he counsel of ' and target is 't'\n",
      "when input is 'he counsel of t' and target is 'h'\n",
      "when input is 'he counsel of th' and target is 'e'\n",
      "********\n",
      "when input is 'l' and target is 'a'\n",
      "when input is 'la' and target is 'c'\n",
      "when input is 'lac' and target is 'e'\n",
      "when input is 'lace' and target is 's'\n",
      "when input is 'laces' and target is ','\n",
      "when input is 'laces,' and target is ' '\n",
      "when input is 'laces, ' and target is 'b'\n",
      "when input is 'laces, b' and target is 'e'\n",
      "when input is 'laces, be' and target is 'c'\n",
      "when input is 'laces, bec' and target is 'a'\n",
      "when input is 'laces, beca' and target is 'u'\n",
      "when input is 'laces, becau' and target is 's'\n",
      "when input is 'laces, becaus' and target is 'e'\n",
      "when input is 'laces, because' and target is ' '\n",
      "when input is 'laces, because ' and target is 't'\n",
      "when input is 'laces, because t' and target is 'h'\n",
      "********\n"
     ]
    }
   ],
   "source": [
    "processed_file_path = 'data/processed/kjv.txt'\n",
    "documents = read_file(processed_file_path)\n",
    "\n",
    "# concat all documents into one string\n",
    "documents = [\"\".join(documents)]\n",
    "print(\"all docuents are a single string\", len(documents))\n",
    "\n",
    "tokenizer = Tokenizer(None, vocab_file)\n",
    "\n",
    "documents_tensor = [torch.tensor(tokenizer.encode(doc), dtype=torch.long) for doc in documents][0]\n",
    "\n",
    "xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "print(\"input: \", xb)\n",
    "print(xb.shape)\n",
    "print(\"output: \", yb)\n",
    "print(yb.shape)\n",
    "\n",
    "print(\"-----\")\n",
    "\n",
    "for b in range(batchsize):\n",
    "    for t in range(context):\n",
    "        time_context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is '{tokenizer.decode(time_context.tolist())}' and target is '{tokenizer.decode([int(target)])}'\")\n",
    "    \n",
    "    print(\"********\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram Language Model\n",
    "\n",
    "The loss is cross entropy loss and the vocabulary size is the target number of classes. <br>\n",
    "This is so chosing  because we predict one of the tokens in our vocabulary at each time step. <br><br>\n",
    "\n",
    "For the bigram model:\n",
    "- We set the embedding size is our number of classes. In a real network, the inputs are modified such that the last layer equals the vocabularize size\n",
    "- Our embedding size is also our vocabulary size. Our logits become (batch_size, vocab_size, vocab_size).\n",
    "\n",
    "**For computational purposes** <br>\n",
    "You can visualize this as each row corresponds to the embedding of each token. <br> Each token is a cell value in the orignal batch input.\n",
    "- input reshape => (batch * num_tokens_in_sequence or time dimension, embedding_dims or classes )\n",
    "- target shape => (batch * num_tokens_in_sequence or time dimension)\n",
    "\n",
    "\n",
    "### Generate\n",
    "To generate:\n",
    "1. We select the last time step\n",
    "2. Sample from a multinomial distribution\n",
    "3. Add the generated input to the input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dims):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        embedding layer is basically a dense layer with the following differences:\n",
    "            1. the input is a one-hot encoded tensor\n",
    "            2. since we want to embed the input, the size of the one-hot encoded tensor\n",
    "                is the same as the entire vocabulary. We wanna dedicate a single position\n",
    "                in the tensor to a token. This makes the dense layer weights effectively \n",
    "                a lookup table.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        # logits shape (batch, num_tokens_in_sequence or time dimension, embedding_dims)\n",
    "        logits = self.embedding_table(idx)\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(idx, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 63])\n",
      "tensor(4.5577, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.vocabulary)\n",
    "# because it is a bigram mode, embedding_dims = vocab_size  \n",
    "m = BigramLanguageModel(vocab_size, embedding_dims=vocab_size).to(device)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a, and Joah, unt vstFqtuES',\n",
       " 'y will not do thof)?FLLMEi',\n",
       " 'h their wings ov:OZzvsd)Z,',\n",
       " 'onourable man; aUgDpcKweol',\n",
       " 'ed one another aiT:;ty\\nR-G',\n",
       " 'ated, and that ho,mpc(pc C',\n",
       " 'he counsel of thY,Q(z:res(',\n",
       " 'laces, because taZSME.NvkE']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.generate_and_show(xb, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.669612884521484\n",
      "900 3.621502161026001\n",
      "1800 3.2125680446624756\n",
      "2700 2.626553773880005\n",
      "3600 2.62990403175354\n",
      "4500 2.5950770378112793\n",
      "5400 2.5170986652374268\n",
      "6300 2.3890011310577393\n",
      "7200 2.235649347305298\n",
      "8100 2.197721481323242\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(m.parameters(), lr=1e-3)\n",
    "epochs = 9000\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\nIse the whe ill, threhe ccandv; Lofre t u, K, t: h.\\nI t, Isan winto ore al thyprs, oushe ad, t oven f Bant to he t hanthat; pan, f, omal allteypors heinors, f an thef Is tevoun, ais kldswigeve beshe pis AIsanshixll the?\\nThes yn aghe, nd sitind toabes llland; d gered y t ckoo flt y asrd o toneahid wo coon f wngnt thethe this hamecalse, t ko thand theyghethigicanunteantel Wmenedthim.\\nThir wilind I qun ascothaheH'u mb nd:\\nEgound, bulQm I inhalded VWU!Sped f ched, I me athe scud lla touthavo Istol ce wee ameatcharyot thagh; hale ainharethomp avilel rrellee wheashey s aun, t the t h h LORDanwe wil B'Ywalan het tsecof he tFprathramend? nd Jou fthos thesth hir thashend athive t se thiend the he ales sse bred we thesheth m f ornsamapthanhe tungd the witbuth weherounghokelen atheHORa hed thil sthah hiKnororwhther chelithe, lin t dind -; ththavethand atre blimas atowired t LORDagiLORD; assk: al marod the brof Wx, sgund ceZRD thellis e toserereveangle of theom towop, leche poughit Is Inentess ge \"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampel_input = torch.zeros((1,1), dtype=torch.long).to(device)\n",
    "m.generate_and_show(sampel_input, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self attention mathematics @ t=50\n",
    "\n",
    "He used a triangular matrix  to find the average of previous time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Head(nn.Module):\n",
    "    \"\"\" One self attention head \"\"\"\n",
    "    def __init__(self, n_embed, head_size):\n",
    "        super().__init__()\n",
    "        self.query_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.key_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value_layer = nn.Linear(n_embed, head_size, bias=False)\n",
    "        # lower triangular matrix of a torch.ones\n",
    "        self.register_buffer('mask', torch.tril(torch.ones(context, context)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key_layer(x)\n",
    "        q = self.query_layer(x)\n",
    "        \n",
    "        # compute self attention scores (\"affinities\")\n",
    "        wei = q@k.transpose(-2, -1) * C**-0.5\n",
    "        # stop at time step just to be efficient\n",
    "        wei = wei.masked_fill(self.mask[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value_layer(x)\n",
    "        out = wei@v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This is what I always misunderstood.\n",
    "    Each Head takes in the full embedding size as input and outputs (embedding/ n_heads)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_embed, num_heads):\n",
    "        super().__init__()\n",
    "        self.head_size = n_embed // num_heads\n",
    "        # takes in the full embedding  as input\n",
    "        self.heads = nn.ModuleList([Head(n_embed, self.head_size) for _ in range(num_heads)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "class BigramLanguageAttentionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    (embedding_dims) -> (n_heads * (embedding_dims // n_heads)) ->  vocab_size\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dims, num_heads):\n",
    "        super(BigramLanguageAttentionModel, self).__init__()\n",
    "        self.head_size = embedding_dims // num_heads\n",
    "        # embed the entire vocabulary size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embedding_dims)\n",
    "        # embed the position of the word in the context\n",
    "        self.positional_embedding_table = nn.Embedding(context, embedding_dims)\n",
    "        self.sa_head = MultiHeadAttention(embedding_dims, num_heads)\n",
    "        self.lm_head = nn.Linear(embedding_dims, vocab_size)\n",
    "\n",
    "    \n",
    "    def forward(self, idx, targets):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        token_embed = self.token_embedding_table(idx)\n",
    "        pos_embed = self.positional_embedding_table(torch.arange(context, device=device))\n",
    "        x = token_embed + pos_embed\n",
    "        x = self.sa_head(x)\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # we use view to retain the ordering of the vectors instead of reshape\n",
    "            logits = logits.view(batchsize * context,  -1)\n",
    "            targets = targets.view(batchsize * context)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "            \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            # ensure that when generating, we have a maximum of the length of the context being pedicted\n",
    "            idx_cond = idx[:, -context:]\n",
    "            logits, _ = self.forward(idx_cond, None)\n",
    "            logits_for_last_time_step = logits[:, -1, :]\n",
    "            probs = F.softmax(logits_for_last_time_step, dim=1)\n",
    "            # sample from a multinomial distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append to input\n",
    "            idx = torch.cat([idx, idx_next], dim=1)\n",
    "\n",
    "        return idx\n",
    "\n",
    "\n",
    "    def generate_and_show(self, idx, max_new_tokens):\n",
    "        out = self.generate(idx, max_new_tokens)\n",
    "        return [tokenizer.decode(x.tolist()) for x in out]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 4.148254871368408\n",
      "1000 2.3247289657592773\n",
      "2000 2.1632871627807617\n",
      "3000 2.14971923828125\n",
      "4000 2.164445638656616\n",
      "5000 2.031420946121216\n",
      "6000 2.0891637802124023\n",
      "7000 1.933702826499939\n",
      "8000 2.0308926105499268\n",
      "9000 1.905373215675354\n"
     ]
    }
   ],
   "source": [
    "batchsize = 32\n",
    "context = 24\n",
    "n_heads =  4\n",
    "embedding_dims = 20\n",
    "lr = 1e-3\n",
    "\n",
    "m_attention = BigramLanguageAttentionModel(vocab_size, embedding_dims, n_heads).to(device)\n",
    "optimizer = torch.optim.Adam(m_attention.parameters(), lr=lr)\n",
    "epochs = int(1e4)\n",
    "\n",
    "for steps in range(epochs):\n",
    "    xb, yb = generate_batch(documents_tensor, batchsize, context)\n",
    "    logits, loss = m_attention(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps % (epochs/ 10) == 0:\n",
    "        print(steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(input_string):\n",
    "    input_string = tokenizer.encode(input_string)\n",
    "    inp_size = len(input_string)\n",
    "    if inp_size < context:\n",
    "        input_string = [0] * (context - inp_size) + input_string\n",
    "\n",
    "    return torch.tensor(input_string, dtype=torch.long).to(device).reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nJesush din them idfere theid feriand sten a intakedeit anmu wher, and on fat dame presh, te mevighth sing Wuemer asart me; sand etheirgoy gile thee forrye hather.\\nThalkieghir yonesh to wone Ameroust bn tha the heakin.\\nStheon, or anto cacows alin twore, hefor mo chieshe forly, frothe laclaikne of Issoprl.\\nAnd ast, and sinthereagm agol.\\nAnd Gidepire yro ther ale wereth the me name igree: ow his whimnsayd scouah said be? when thired.\\nBesend; be yourd nyt fese yontugh ye hur wiss and ed, istten shieds hee, ther of to with shal ofs.\\nThim of Jamat: whert weos of I Cuetr hece itrecom, thoul astaitilventamed ferfor yep and beorfyoall ofren tanomin whe se, and prt omiand.\\nFusem bakivitweth the thaund; tO theey housht sonounts inentinert? I thers cae insty ahauds name shir i? yook, thert igan mfowhong bu, out cakous? pas off lanjust;) ofetce counted rowes hey the worus.\\nAnd theres?\\nS at ind yar;\\nRer wrons dome micmen.\\nThemsto thento I ing aveoto Jeudee sin Sof amageast asd und, me.\\n(Lower, and tre ofb; win that ibe was one off holiked weo the she net his alliu, rifrsent whis, and it not.\\nAnd I the the mnins the mout ite andt hate our anto romande, in that se of wreipst seiclerete sein I ith goive heunge:\\nTo seallle thy the pith and agar wired hised: but on whil ahe spey arom whey thato upill no therait u shee.\\nThy ecen and ang fthow hord, cooup, rougrgos so thianimsay shalr to toif sher tine detar asberkene:, ox uprsty Has the sleall werene them areste wich a srun himgh, and le yery unto the for thentathegoertthe the hefory. Saw ond sos wand bes, orepils, as sence pthe wand it hushte, and oncess, and ofn ingtnot corser yo; rought toit.\\nAfy and; and houglde saiders row iredsso, er, sin.\\nAnd bon ake and jod, ond is on.\\nNorust him he the mre a onunto miselae the she pre the wou iten node abesin of the bigmku ind, on fragivt, and of sowar al; he me' wel perl: for the marsbe forever rous thoud.\\nWhens wicelre amety gar boilet of the ree chy are stre Lof anman thy ny,aey bevigaveves shes in peacore, orof se theand cat mess, and afel geiret sonceas the for, won doniugh, amit fhaur.\\nSking amin hist ifrrans ant; ayet.\\nTher just them foringciet sibe seve.\\nNo somitrious, eeth, was the Leagr shenter.\\nThe hen, couvin htoed naget civel thand ant she see Lered.\\nAnd ther?\\nAnd alt tace ofenc e.\\nSof Eljed the de with they LORD: wel, him, hath thath ings aby tos ther.\\nWhe he Lad, and fow sal selnoto tronss wey, hat hist se, he fem aceso and;\\nAnd not.\\nAt ons hat the bay nevigwrovede, he sarch thecy a the dol: tand tunto et, and Jombrun gentell rentk athe allel acome men: son, ther node to nencmecy tefstabus, ingremne ste withsaid Gad.\\nGagnow befn, und of thake mamought hat geath not, beendsed on be coren has arde ubeem, at on Elced wis for a Wheabrssayd were.\\nAnd stery gal andewas on, out him; ghes.\\nYey upeacentno shior ree forinem anromly beand: wir my wore witos thestte thean: ou cuh be, wamndomok.\\nThig Is agesthied nat the of saed ceen ceant and yours: in onare rtaitenat are Lould cithey you, roust sirt, ous thee und.\\nAnd: and soprunt.\\nAnd tof willen healir the, and theret;\\nUplay:\\nWhrom ticeriph fien hresemen whreh: hear wrthe shalker soberefor, the give.\\nSow of thele of thut the himinetesesand the ent u adnjedh sin as the pcanks; and yous heg eincm, on Jeastif was neand, thir the eather usonings thatheren fort wandnfor the lord; fepn hat hath and.\\nAnd the baman wheld not hotleree hat on, wilded and, in cor has insen ay sid gal.\\nWheriom thern, bol, oun alvom, uthes tifir loosthere, nin of throuste be fireshs, whear teemigh De, in Hall shial of telle offer he unck: heaese rous of ryingen, tip?\\nJahes ake, of shurch ous fore ithothy tand with to mon anots? und Ish beirelertonemne.\\nAnd the whe shae of meamedres bultaughtr.\\nThad m mash, the Ely, and asets agico.\\nAnd asherak mysudrote ats foor, tet; lerebe hing thy sain thurof LORD: the saint them: thered is pon awth, pohisis feasent and the eve; lof Hi the ne heferneshe by thery on; her the the megoke docstren wethate: but rod, sh; oad mon. And songth, and igildent the aldeit thouch Jendeen,\\nAnd bours be devt ave houput watut hears not O mase the witak, his inged ith out there kever; cowers was no wael Lonen? ow pon of thu sor thates eve wath wis orias shey is not pent yoe meah cORent ughirgs of say, taughte the feath Nall cigand Nits swouldsaide menrds dalke: and ther kered Swroueazomay, uity for he orugh thown the LORD the nevawrten be shall kes nores: und hat of thess, aught kvemedeld, and per; on sard the Lealll Ciron and hold man cren, a bersee hisand nothem ceiain fans TI of aman cepnd cef mte of I was cain ibved the ble gonke eve LORD,, ens her, him lon, be ga, yeand the theam ingelos yesh thon the.\\nAnd wuth I andake them; and coued dove sto meveit, and no chish.\\nAnd alll as alemr, ser not with, gegefshallgo beh heavent of thavistk alt and God beter ancabaregy acede the siucar Weth se me asdak oughtcee of bent I fat amanden hat amnzot oughtro.\\nAnd the LORD uper.\\nThou gay andent sry womethm of ast ce breesty wornesscin whes thecorgss holl, er ande, and eyo; eforsflor the beldirom to roweird hech brughtett Groy on sher.\\nWheselle peuto monersh, ons of thee ish his wrh, whes,) of acress wy them. LORD there shuld of Daw on ings pevosiair hither.\\nAnd husincenc ee nes the frento therethe houth eversed houch of Seaherelssee the hae yo.\\nSough vounte stie.\\nAgrowfly, houstt of theledl gto bery It, uns thown at nougu hald wame sine thesary,: pay ouornsar and be the mand, whorderse la jedated po thenth dand ousaidthe biedahen of hous mbep arnow: mi, blops warrs, ror, wa thus amrd, and of Jodis whem, even to sha, and cabminge at of theer, ton na thererses?\\nThem and ast bos cabedl.\\nWh acy yoves, feaitat und of then shae they ingenens, rous, and it shar she ane wor; for, Kvese, the thi blrainsl amopses reaw oreghabe beforedrt, ok nerts tholtye agon, yar Dar hust even coukete, yougo: thembese has unte upss: on ce yoer al, Hart aringu, on me, there shalll they robent.\\nW hincor a und, whe mith dis undng cing, erin Mal beficesr the wah stitr ofor wyererk, on thes boud agentife u ace offfeer thar wa drouw hifor to for athere shiy wles:\\nAfoh, sok, cexer me:\\nTher, puched ther, wher, cand that, evid them, Itar pral breachis of thim they rongter, eey: his aid mocom amredl dmbon theme wah aver det.\\nHen Apart nod, u ind, on mo ans, and wion in cam, fwhe ceering bes romais: not sentren pophe e, the the tion seved ast and aptets; or upether hath mans aidestr the then the mins his unto Sther of ben campen rent ond thoumrie; and unto they LORD, ther ye fis thim Lone.\\nAnd ald thes: thery thurteshath milvery, and seim, not is of sen not thed of Mey, ord ofmave metekop; ovese ingto ther is he sigite this, and the wity Srons all veary heave Le bewlordiex assfribem nacode ast Chath comemede igon the of the theps then ary hillsei? and cisken ong so whers. And mon the budersaid, whe ther.\\nAnd Wult cinse, ord beath.\\nAnd the alldof su; por pesen the halarl, the not ughte mat the srond unss ist.\\nLor: wor, hold mefaly tin, and cepthe therleshuindtk, bre ince mtrold, sond sogher masabales thes? propwerchoust houser face.\\nAnd ast, win thes unto thuret as feirlfbtar bethatar Ford hall jumity fior to of I shy sance, ror ing? and curre wer, und the sigar I come as of sand le se;\\nAnd for reau had me.\\nThe that thes incgaivis the incse akfor veee, Des segropicar' unto misrdist aid meng.\\nAnd ren of wire peo heur ande ith; OLORD, and beelliables, ingeit seal Bathe oves wes his nows th, mond.\\nWher hashert, and the psts, and eved: for, und in weild ovens.\\nUnd toifor thy itereath and thintuplprian the de, of the in, to tout of the aginld made asto ofredest shimsiqus hicilde gmdethe dus sher, and a the reunto they of LORD, for won atth ther peorifed.\\nS: the meayd heallest of theree antes, resh, eyord shande note mparcestm they wingted;\\nAnd but to wedh to ctase me meven wetrod,ar ast of is cor, and of as sHiore low the, and usall sefypriayvep dandm whram ijudinte were histhe degfered ist shelvce chie hinento rouses yaiasd, son ansaar cis vile avet, yoo won andd himed it I oniqrein the Tthe pkeonot the trothe withem sem lilder, and corse thery mey and begerr hen thu, on pcey houdrere kys.\\nCleced ere sshe nis abod, and min a een?\\nSes, and ye me: the dey.\\nFor thernad: son a ganto twer meatusided yoo thes eyour re: hat Phaus in of oult eand, he athey hat afrifartane stwran of Inor thicored shely.\\nPreiomoe thredinnest dones al disere ney chis, and ste cefragRill houto dous ae bovat forlse pere ounto the cinsh our they min to the Py thur, of Bveherialls: heat, the ant, juds fe Paemonelt?\\nWhioundgen bven ther the Challamldarndese shezphops of Mhentarl the.\\nAnd thaz, cayd peth go Challl andmedrous, whey anto hour, and lriate acoumionujwals ougipll untaver the knotlde' sook dop as' the ame shefor thilde ass wris ult shemy whe that ceesen anttion me home, and homabre bres unist senm: ye, uredeak I clo be nany:\\nThe damnppnot werst shaift Ate, and do hound agan, and GO por eathe Bugerst.\\nAnd hou, ingesesh, or.\\nAm, forsa.\\nBuriforfer anaintatrl hout ante wite thas orhad shea.\\nAnd welntar thilvelers billese anto himedercedin sare darfell Gevachring the kefld.\\nNowhedrow ongs.\\nBle with thath amestderedr, shellefap sen rid.\\nIfrra.\\nWhsem hous mamzarts the went, prilt.\\nThear uppronusirs renentar bee deans daittamoteired hadought goldseree here: sthy e the aring:\\nAnd barther poret becapar?\\nWhes.\\nThe danto hey LORD the here of Datt them thiast the strerved sher, lid lomadt of the oubert anceng, ough, untart se, of our tupringsentwo ivesh, hast ceplast: mof he sal, thes domuse mivim, and thereee the ster, eam them wing Is of thingspercy ass se of the rif ton to confs go, et;\\nHof tous of the unter andocosemrabr seard ught kiblervedeld, so.\\nAnd hat I shallvelsicare hrou be not to the thpeon ashaly?\\nAnd enogu ahe Larveren hurty.\\nSel the evitow O thecen ants.\\nAnd cey fear bee lil saintifn the thiew.\\nHes wires sheay mion anto tiffor has prown Ashewaem mas itwhraizs hirn, a\"]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_string = \"Jesus\"\n",
    "sampel_input = encode_input(input_string)\n",
    "m_attention.generate_and_show(sampel_input, 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('nano-gpt-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9129c7367580ac82d5c613b410f799afbd954f1c5e656960793a9207aa7dd59b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
